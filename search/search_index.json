{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the OmniRec Documentation","text":""},{"location":"API_references/","title":"API References","text":""},{"location":"API_references/#omnirec.RecSysDataSet","title":"<code>RecSysDataSet</code>","text":"<p>               Bases: <code>Generic[T]</code></p> Source code in <code>src\\omnirec\\recsys_data_set.py</code> <pre><code>class RecSysDataSet(Generic[T]):\n    _folds_file_pattern = re.compile(r\"(\\d+)\\/(?:train|val|test)\\.csv\")\n\n    def __init__(\n        self, data: Optional[T] = None, meta: _DatasetMeta = _DatasetMeta()\n    ) -&gt; None:\n        if data:\n            self._data = data\n        self._meta = meta\n\n    @staticmethod\n    def use_dataloader(\n        data_set_name: str,\n        raw_dir: Optional[PathLike | str] = None,  # TODO: Name that right\n        canon_path: Optional[PathLike | str] = None,  # TODO: Name that right\n        force_download=False,\n        force_canonicalize=False,\n    ) -&gt; \"RecSysDataSet[RawData]\":\n        \"\"\"Loads a dataset using a registered DataLoader. If not already done the data set is downloaded and canonicalized. \n        Canonicalization means duplicates are dropped, identifiers are normalized and the data is saved in a standardized format.\n\n        Args:\n            data_set_name (str): The name of the dataset to load (must be registered in the DataLoader registry).\n            raw_dir (Optional[PathLike | str], optional): Target directory where the raw data is stored. If not provided, the data is downloaded to the default raw data directory (_DATA_DIR).\n            canon_path (Optional[PathLike | str], optional): Path where the canonicalized data should be saved. If not provided, the data is saved to the default canonicalized data directory (_DATA_DIR / \"canon\").\n            force_download (bool, optional): If True, forces re-downloading of the raw data even if it already exists. Defaults to False.\n            force_canonicalize (bool, optional): If True, forces re-canonicalization of the data even if a canonicalized file exists. Defaults to False.\n\n        Returns:\n            RecSysDataSet[RawData]: The loaded dataset in canonicalized RawData format.\n\n        Example:\n            ```Python\n            # Load the MovieLens 100K dataset using the registered DataLoader\n            # Download the raw data to the default directory and save the canonicalized data to the default path\n            dataset = RecSysDataSet.use_dataloader(data_set_name=\"MovieLens100K\")\n            ```\n        \"\"\"\n        dataset = RecSysDataSet[RawData]()\n\n        dataset._meta.name = data_set_name\n\n        if canon_path:\n            dataset._meta.canon_pth = Path(canon_path)\n        else:\n            canon_dir = _DATA_DIR / \"canon\"\n            canon_dir.mkdir(parents=True, exist_ok=True)\n            dataset._meta.canon_pth = (canon_dir / data_set_name).with_suffix(\".csv\")\n        if dataset._meta.canon_pth.exists() and not (\n            force_canonicalize or force_download\n        ):\n            logger.info(\n                \"Canonicalized data set already exists, skipping download and canonicalization.\"\n            )\n            dataset._data = RawData(pd.read_csv(dataset._meta.canon_pth))\n            return dataset\n\n        if raw_dir:\n            dataset._meta.raw_dir = Path(raw_dir)\n\n        dataset._data = RawData(\n            registry._run_loader(data_set_name, force_download, dataset._meta.raw_dir)\n        )\n        dataset._canonicalize()\n        return dataset\n\n    # TODO: Expose drop dup and norm id params to public API somehow\n    def _canonicalize(self, drop_duplicates=True, normalize_identifiers=True) -&gt; None:\n        # HACK: We might implement it for the other data variants if needed\n        if not isinstance(self._data, RawData):\n            logger.error(\"Cannot canonicalize non raw data, aborting!\")\n            return\n        start_time = time()\n        logger.info(\"Canonicalizing raw data...\")\n\n        if drop_duplicates:\n            self._drop_duplicates()\n        if normalize_identifiers:\n            self._normalize_identifiers()\n        # self.check_and_order_columns() # TODO: Ask Lukas about the complex checking logic in the OG. Why the ordering, since columns are named?\n        # self.check_and_convert_data_types() # TODO: Check back with Lukas, this might be the wrong place to do that, since after writing/loading from csv dtypes are different again: Result: Do that in adapters! Be careful, str may work, but lib may do it as category.\n        stop_time = time()\n        logger.info(f\"Canonicalized raw data in {(stop_time - start_time):.4f}s.\")\n        logger.info(f\"Saving to {self._meta.canon_pth}...\")\n        self._data.df.to_csv(self._meta.canon_pth, index=False)\n\n    def _drop_duplicates(self) -&gt; None:\n        # HACK: We might implement it for the other data variants if needed\n        if not isinstance(self._data, RawData):\n            logger.error(\"Cannot drop duplicated on non raw data, aborting!\")\n            return\n        logger.info(\"Dropping duplicate interactions...\")\n        logger.info(f\"Number of interactions before: {self.num_interactions()}\")\n        self._data.df.drop_duplicates(\n            subset=[\"user\", \"item\"], keep=\"last\", inplace=True\n        )\n        logger.info(f\"Number of interactions after: {self.num_interactions()}\")\n\n    def _normalize_identifiers(self) -&gt; None:\n        # HACK: We might implement it for the other data variants if needed\n        if not isinstance(self._data, RawData):\n            logger.error(\"Cannot normalize identifiers on non raw data, aborting!\")\n            return\n        logger.info(\"Normalizing identifiers...\")\n        for col in [\"user\", \"item\"]:\n            unique_ids = {\n                key: value for value, key in enumerate(self._data.df[col].unique())\n            }\n            self._data.df[col] = self._data.df[col].map(unique_ids)\n        logger.info(\"Done.\")\n\n    def replace_data(self, new_data: R) -&gt; \"RecSysDataSet[R]\":\n        new = cast(RecSysDataSet[R], copy.copy(self))\n        new._data = new_data\n        return new\n\n    # region Dataset Statistics\n\n    @overload\n    def num_interactions(self: \"RecSysDataSet[RawData]\") -&gt; int: ...\n\n    @overload\n    def num_interactions(self: \"RecSysDataSet[SplitData]\") -&gt; dict[str, int]: ...\n\n    @overload\n    def num_interactions(\n        self: \"RecSysDataSet[FoldedData]\",\n    ) -&gt; dict[int, dict[str, int]]: ...\n\n    @overload\n    def num_interactions(\n        self: \"RecSysDataSet[T]\",\n    ) -&gt; int | dict[str, int] | dict[int, dict[str, int]]: ...\n\n    def num_interactions(self):\n        if isinstance(self._data, RawData):\n            return len(self._data.df)\n        elif isinstance(self._data, SplitData):\n            return {split: len(df) for split, df in self._data.iter_splits()}\n        elif isinstance(self._data, FoldedData):\n            return {\n                fold_num: {split: len(df) for split, df in fold_data.iter_splits()}\n                for fold_num, fold_data in self._data.folds.items()\n            }\n        else:\n            logger.error(\"Unknown data variant!\")\n            return -1\n\n    def min_rating(self) -&gt; float | int:\n        # TODO: # HACK: I feel like these should easily implemented\n        if not isinstance(self._data, RawData):\n            logger.error(\"Cannot get min_rating on non raw data, aborting!\")\n            return -1\n        return self._data.df[\"rating\"].min()\n        # TODO: Do we need that line: ?\n        # if self.feedback_type == \"explicit\" else None\n\n    def max_rating(self) -&gt; float | int:\n        # TODO: # HACK: I feel like these should easily implemented\n        if not isinstance(self._data, RawData):\n            logger.error(\"Cannot get max_rating on non raw data, aborting!\")\n            return -1\n        return self._data.df[\"rating\"].max()\n        # TODO: Do we need that line: ?\n        # if self.feedback_type == \"explicit\" else None\n\n    # endregion\n\n    # region File IO\n\n    # TODO: Logging in save function\n    # TODO: check if path already exists\n    # TODO: Error handling: logger.critical and sys.exit(1) if any step causes an error\n    def save(self, file: str | PathLike):\n        \"\"\"Saves the RecSysDataSet object to a file with the default suffix .rsds.\n\n        Args:\n            file (str | PathLike): The path where the file is saved.\n        \"\"\"\n        file = Path(file)\n        if not file.suffix:\n            file = file.with_suffix(\".rsds\")\n        with zipfile.ZipFile(file, \"w\", zipfile.ZIP_STORED) as zf:\n            if isinstance(self._data, RawData):\n                with zf.open(\"data.csv\", \"w\") as data_file:\n                    self._data.df.to_csv(data_file, index=False)\n                zf.writestr(\"VARIANT\", \"RawData\")\n            elif isinstance(self._data, SplitData):\n                for filename, data in zip(\n                    [\"train\", \"val\", \"test\"],\n                    [self._data.train, self._data.val, self._data.test],\n                ):\n                    with zf.open(filename + \".csv\", \"w\") as data_file:\n                        data.to_csv(data_file, index=False)\n                zf.writestr(\"VARIANT\", \"SplitData\")\n            elif isinstance(self._data, FoldedData):\n                # TODO: Leveraging the new SplitData.get method this can be simplified:\n                def write_fold(fold: int, split: str, data: pd.DataFrame):\n                    with zf.open(f\"{fold}/{split}.csv\", \"w\") as data_file:\n                        data.to_csv(data_file, index=False)\n\n                for fold, splits in self._data.folds.items():\n                    write_fold(fold, \"train\", splits.train)\n                    write_fold(fold, \"val\", splits.val)\n                    write_fold(fold, \"test\", splits.test)\n\n                zf.writestr(\"VARIANT\", \"FoldedData\")\n\n            else:\n                logger.critical(\n                    f\"Unknown data variant: {type(self._data).__name__}! Aborting save operation...\"\n                )\n                sys.exit(1)\n\n            zf.writestr(\"META\", json.dumps(asdict(self._meta), default=str))\n            # HACK: Very simple versioning implementation in case we change anything in the future\n            zf.writestr(\"VERSION\", \"1.0.0\")\n\n    # TODO: Check file exists\n    # TODO: Error handling: logger.critical and sys.exit(1) if any step causes an error\n    @staticmethod\n    def load(file: str | PathLike) -&gt; \"RecSysDataSet[T]\":\n        \"\"\"Loads a RecSysDataSet object from a file with the .rsds suffix.\n\n        Args:\n            file (str | PathLike): The path to the .rsds file.\n\n        Returns:\n            RecSysDataSet[T]: The loaded RecSysDataSet object.\n        \"\"\"\n        with zipfile.ZipFile(file, \"r\", zipfile.ZIP_STORED) as zf:\n            version = zf.read(\"VERSION\").decode()\n            # HACK: Very simple versioning implementation in case we change anything in the future\n            if version != \"1.0.0\":\n                logger.critical(f\"Unknown rsds-file version: {version}\")\n                sys.exit(1)\n\n            variant = zf.read(\"VARIANT\").decode()\n\n            if variant == \"RawData\":\n                with zf.open(\"data.csv\", \"r\") as data_file:\n                    data = RawData(pd.read_csv(data_file))\n            elif variant == \"SplitData\":\n                dfs: list[pd.DataFrame] = []\n\n                for filename in [\"train\", \"val\", \"test\"]:\n                    with zf.open(filename + \".csv\", \"r\") as data_file:\n                        dfs.append(pd.read_csv(data_file))\n\n                data = SplitData(dfs[0], dfs[1], dfs[2])\n            elif variant == \"FoldedData\":\n                folds: dict[int, SplitData] = {}\n\n                for p in zf.namelist():\n                    match = RecSysDataSet._folds_file_pattern.match(p)\n                    if not match:\n                        continue\n\n                    fold = match.group(1)\n                    folds.setdefault(\n                        int(fold), SplitData(*[pd.DataFrame() for _ in range(3)])\n                    )\n\n                # TODO: Leveraging the new FoldedData.from_split_dict method this can be simplified:\n                def read_fold(fold: int, split: str) -&gt; pd.DataFrame:\n                    with zf.open(f\"{fold}/{split}.csv\", \"r\") as data_file:\n                        return pd.read_csv(data_file)\n\n                for fold, split_data in folds.items():\n                    split_data.train = read_fold(fold, \"train\")\n                    split_data.val = read_fold(fold, \"val\")\n                    split_data.test = read_fold(fold, \"test\")\n\n                data = FoldedData(folds)\n            else:\n                logger.critical(\n                    f\"Unknown data variant: {variant}! Aborting load operation...\"\n                )\n                sys.exit(1)\n\n            meta = zf.read(\"META\").decode()\n            meta = _DatasetMeta(**json.loads(meta))\n            return cast(RecSysDataSet[T], RecSysDataSet(data))\n</code></pre>"},{"location":"API_references/#omnirec.RecSysDataSet.load","title":"<code>load(file)</code>  <code>staticmethod</code>","text":"<p>Loads a RecSysDataSet object from a file with the .rsds suffix.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>The path to the .rsds file.</p> required <p>Returns:</p> Type Description <code>RecSysDataSet[T]</code> <p>RecSysDataSet[T]: The loaded RecSysDataSet object.</p> Source code in <code>src\\omnirec\\recsys_data_set.py</code> <pre><code>@staticmethod\ndef load(file: str | PathLike) -&gt; \"RecSysDataSet[T]\":\n    \"\"\"Loads a RecSysDataSet object from a file with the .rsds suffix.\n\n    Args:\n        file (str | PathLike): The path to the .rsds file.\n\n    Returns:\n        RecSysDataSet[T]: The loaded RecSysDataSet object.\n    \"\"\"\n    with zipfile.ZipFile(file, \"r\", zipfile.ZIP_STORED) as zf:\n        version = zf.read(\"VERSION\").decode()\n        # HACK: Very simple versioning implementation in case we change anything in the future\n        if version != \"1.0.0\":\n            logger.critical(f\"Unknown rsds-file version: {version}\")\n            sys.exit(1)\n\n        variant = zf.read(\"VARIANT\").decode()\n\n        if variant == \"RawData\":\n            with zf.open(\"data.csv\", \"r\") as data_file:\n                data = RawData(pd.read_csv(data_file))\n        elif variant == \"SplitData\":\n            dfs: list[pd.DataFrame] = []\n\n            for filename in [\"train\", \"val\", \"test\"]:\n                with zf.open(filename + \".csv\", \"r\") as data_file:\n                    dfs.append(pd.read_csv(data_file))\n\n            data = SplitData(dfs[0], dfs[1], dfs[2])\n        elif variant == \"FoldedData\":\n            folds: dict[int, SplitData] = {}\n\n            for p in zf.namelist():\n                match = RecSysDataSet._folds_file_pattern.match(p)\n                if not match:\n                    continue\n\n                fold = match.group(1)\n                folds.setdefault(\n                    int(fold), SplitData(*[pd.DataFrame() for _ in range(3)])\n                )\n\n            # TODO: Leveraging the new FoldedData.from_split_dict method this can be simplified:\n            def read_fold(fold: int, split: str) -&gt; pd.DataFrame:\n                with zf.open(f\"{fold}/{split}.csv\", \"r\") as data_file:\n                    return pd.read_csv(data_file)\n\n            for fold, split_data in folds.items():\n                split_data.train = read_fold(fold, \"train\")\n                split_data.val = read_fold(fold, \"val\")\n                split_data.test = read_fold(fold, \"test\")\n\n            data = FoldedData(folds)\n        else:\n            logger.critical(\n                f\"Unknown data variant: {variant}! Aborting load operation...\"\n            )\n            sys.exit(1)\n\n        meta = zf.read(\"META\").decode()\n        meta = _DatasetMeta(**json.loads(meta))\n        return cast(RecSysDataSet[T], RecSysDataSet(data))\n</code></pre>"},{"location":"API_references/#omnirec.RecSysDataSet.save","title":"<code>save(file)</code>","text":"<p>Saves the RecSysDataSet object to a file with the default suffix .rsds.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>The path where the file is saved.</p> required Source code in <code>src\\omnirec\\recsys_data_set.py</code> <pre><code>def save(self, file: str | PathLike):\n    \"\"\"Saves the RecSysDataSet object to a file with the default suffix .rsds.\n\n    Args:\n        file (str | PathLike): The path where the file is saved.\n    \"\"\"\n    file = Path(file)\n    if not file.suffix:\n        file = file.with_suffix(\".rsds\")\n    with zipfile.ZipFile(file, \"w\", zipfile.ZIP_STORED) as zf:\n        if isinstance(self._data, RawData):\n            with zf.open(\"data.csv\", \"w\") as data_file:\n                self._data.df.to_csv(data_file, index=False)\n            zf.writestr(\"VARIANT\", \"RawData\")\n        elif isinstance(self._data, SplitData):\n            for filename, data in zip(\n                [\"train\", \"val\", \"test\"],\n                [self._data.train, self._data.val, self._data.test],\n            ):\n                with zf.open(filename + \".csv\", \"w\") as data_file:\n                    data.to_csv(data_file, index=False)\n            zf.writestr(\"VARIANT\", \"SplitData\")\n        elif isinstance(self._data, FoldedData):\n            # TODO: Leveraging the new SplitData.get method this can be simplified:\n            def write_fold(fold: int, split: str, data: pd.DataFrame):\n                with zf.open(f\"{fold}/{split}.csv\", \"w\") as data_file:\n                    data.to_csv(data_file, index=False)\n\n            for fold, splits in self._data.folds.items():\n                write_fold(fold, \"train\", splits.train)\n                write_fold(fold, \"val\", splits.val)\n                write_fold(fold, \"test\", splits.test)\n\n            zf.writestr(\"VARIANT\", \"FoldedData\")\n\n        else:\n            logger.critical(\n                f\"Unknown data variant: {type(self._data).__name__}! Aborting save operation...\"\n            )\n            sys.exit(1)\n\n        zf.writestr(\"META\", json.dumps(asdict(self._meta), default=str))\n        # HACK: Very simple versioning implementation in case we change anything in the future\n        zf.writestr(\"VERSION\", \"1.0.0\")\n</code></pre>"},{"location":"API_references/#omnirec.RecSysDataSet.use_dataloader","title":"<code>use_dataloader(data_set_name, raw_dir=None, canon_path=None, force_download=False, force_canonicalize=False)</code>  <code>staticmethod</code>","text":"<p>Loads a dataset using a registered DataLoader. If not already done the data set is downloaded and canonicalized.  Canonicalization means duplicates are dropped, identifiers are normalized and the data is saved in a standardized format.</p> <p>Parameters:</p> Name Type Description Default <code>data_set_name</code> <code>str</code> <p>The name of the dataset to load (must be registered in the DataLoader registry).</p> required <code>raw_dir</code> <code>Optional[PathLike | str]</code> <p>Target directory where the raw data is stored. If not provided, the data is downloaded to the default raw data directory (_DATA_DIR).</p> <code>None</code> <code>canon_path</code> <code>Optional[PathLike | str]</code> <p>Path where the canonicalized data should be saved. If not provided, the data is saved to the default canonicalized data directory (_DATA_DIR / \"canon\").</p> <code>None</code> <code>force_download</code> <code>bool</code> <p>If True, forces re-downloading of the raw data even if it already exists. Defaults to False.</p> <code>False</code> <code>force_canonicalize</code> <code>bool</code> <p>If True, forces re-canonicalization of the data even if a canonicalized file exists. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>RecSysDataSet[RawData]</code> <p>RecSysDataSet[RawData]: The loaded dataset in canonicalized RawData format.</p> Example <pre><code># Load the MovieLens 100K dataset using the registered DataLoader\n# Download the raw data to the default directory and save the canonicalized data to the default path\ndataset = RecSysDataSet.use_dataloader(data_set_name=\"MovieLens100K\")\n</code></pre> Source code in <code>src\\omnirec\\recsys_data_set.py</code> <pre><code>@staticmethod\ndef use_dataloader(\n    data_set_name: str,\n    raw_dir: Optional[PathLike | str] = None,  # TODO: Name that right\n    canon_path: Optional[PathLike | str] = None,  # TODO: Name that right\n    force_download=False,\n    force_canonicalize=False,\n) -&gt; \"RecSysDataSet[RawData]\":\n    \"\"\"Loads a dataset using a registered DataLoader. If not already done the data set is downloaded and canonicalized. \n    Canonicalization means duplicates are dropped, identifiers are normalized and the data is saved in a standardized format.\n\n    Args:\n        data_set_name (str): The name of the dataset to load (must be registered in the DataLoader registry).\n        raw_dir (Optional[PathLike | str], optional): Target directory where the raw data is stored. If not provided, the data is downloaded to the default raw data directory (_DATA_DIR).\n        canon_path (Optional[PathLike | str], optional): Path where the canonicalized data should be saved. If not provided, the data is saved to the default canonicalized data directory (_DATA_DIR / \"canon\").\n        force_download (bool, optional): If True, forces re-downloading of the raw data even if it already exists. Defaults to False.\n        force_canonicalize (bool, optional): If True, forces re-canonicalization of the data even if a canonicalized file exists. Defaults to False.\n\n    Returns:\n        RecSysDataSet[RawData]: The loaded dataset in canonicalized RawData format.\n\n    Example:\n        ```Python\n        # Load the MovieLens 100K dataset using the registered DataLoader\n        # Download the raw data to the default directory and save the canonicalized data to the default path\n        dataset = RecSysDataSet.use_dataloader(data_set_name=\"MovieLens100K\")\n        ```\n    \"\"\"\n    dataset = RecSysDataSet[RawData]()\n\n    dataset._meta.name = data_set_name\n\n    if canon_path:\n        dataset._meta.canon_pth = Path(canon_path)\n    else:\n        canon_dir = _DATA_DIR / \"canon\"\n        canon_dir.mkdir(parents=True, exist_ok=True)\n        dataset._meta.canon_pth = (canon_dir / data_set_name).with_suffix(\".csv\")\n    if dataset._meta.canon_pth.exists() and not (\n        force_canonicalize or force_download\n    ):\n        logger.info(\n            \"Canonicalized data set already exists, skipping download and canonicalization.\"\n        )\n        dataset._data = RawData(pd.read_csv(dataset._meta.canon_pth))\n        return dataset\n\n    if raw_dir:\n        dataset._meta.raw_dir = Path(raw_dir)\n\n    dataset._data = RawData(\n        registry._run_loader(data_set_name, force_download, dataset._meta.raw_dir)\n    )\n    dataset._canonicalize()\n    return dataset\n</code></pre>"},{"location":"datasets_overview/","title":"Dataset Overview","text":"<p>The framework includes many built-in datasets. Use the exact name with the <code>use_dataloader</code> function to load a dataset. Here is the comprehensive list with all dataset names:</p> Name Type/Domain Feedback Source Amazon2014Books Books Explicit Amazon/SNAP Amazon2014Electronics Electronics Explicit Amazon/SNAP Amazon2014MoviesAndTv Movies &amp; TV Explicit Amazon/SNAP Amazon2014CdsAndVinyl Music Explicit Amazon/SNAP Amazon2014ClothingShoesAndJewelry Fashion Explicit Amazon/SNAP Amazon2014HomeAndKitchen Home &amp; Kitchen Explicit Amazon/SNAP Amazon2014KindleStore eBooks Explicit Amazon/SNAP Amazon2014SportsAndOutdoors Sports Explicit Amazon/SNAP Amazon2014CellPhonesAndAccessories Phones Explicit Amazon/SNAP Amazon2014HealthAndPersonalCare Health Explicit Amazon/SNAP Amazon2014ToysAndGames Toys &amp; Games Explicit Amazon/SNAP Amazon2014VideoGames Video Games Explicit Amazon/SNAP Amazon2014ToolsAndHomeImprovement Tools Explicit Amazon/SNAP Amazon2014Beauty Beauty Explicit Amazon/SNAP Amazon2014AppsForAndroid Android Apps Explicit Amazon/SNAP Amazon2014OfficeProducts Office Explicit Amazon/SNAP Amazon2014PetSupplies Pets Explicit Amazon/SNAP Amazon2014Automotive Automotive Explicit Amazon/SNAP Amazon2014GroceryAndGourmetFood Grocery Explicit Amazon/SNAP Amazon2014PatioLawnAndGarden Garden Explicit Amazon/SNAP Amazon2014Baby Baby Explicit Amazon/SNAP Amazon2014DigitalMusic Digital Music Explicit Amazon/SNAP Amazon2014MusicalInstruments Instruments Explicit Amazon/SNAP Amazon2014AmazonInstantVideo Video Streaming Explicit Amazon/SNAP MovieLens100K Movies Explicit GroupLens HetrecLastFM Music Implicit HetRec <p>Listing Available Datasets</p> <p>To see all registered datasets:</p> <pre><code>from omnirec.data_loaders.registry import list_datasets\n\navailable_datasets = list_datasets()\nprint(\"Available datasets:\", available_datasets)\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":"<p>In this guide we will present how to install OmniRec and to design and implement your first experiments with it.</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>First you need to setup a python environment at least python &lt;\\version&gt; and install the OmniRec libary by running &lt;\\command&gt;.</p>"},{"location":"getting_started/#loading-datasets","title":"Loading datasets","text":"<p>Central part of the OmniRec library is the <code>RecSysDataSet</code> class. It loads the data in an object of this class by calling the static <code>use_dataloader()</code> function that return a <code>RecSysDataSet</code> object. If provided a registered data set name <code>use_dataloader()</code> downloads the data set, removes duplicates and normalizes the identifiers:</p> <p><pre><code>from OmniRec import RedSysDataSet\ndata = RecSysDataSet.use_dataloader(data_set_name=\"MovieLens100K\")\n</code></pre> We provide a more detailed documentation here.</p>"},{"location":"getting_started/#preprocessing-datasets","title":"Preprocessing Datasets","text":"<p>We can now apply the desired preprocessing steps to the data. The easiest way is to define a preprocessing pipeline:</p> <pre><code>from omnirec.preprocess.core_pruning import CorePruning\nfrom omnirec.preprocess.feedback_conversion import MakeImplicit\nfrom omnirec.preprocess.pipe import Pipe\nfrom omnirec.preprocess.split import UseCrossValidation\n\npipe = Pipe[FoldedData](\n    Subsample(sample_size=0.1),\n    MakeImplicit(threshold=3),\n    CorePruning(core=5),\n    UserCrossValidation(num_folds=5, validation_size=0.1),\n)\ndata = pipe.process(data)\n</code></pre> <p>The <code>pipe.process()</code> function interatively executes the preprocessing steps. </p> <p>Alternatively, this can be done step by step by creating a single preprocessing step and calling <code>process()</code> on it:</p> <pre><code>step = Subsample(sample_size=0.1)\ndata = step.process(dataset=data)\n</code></pre> <p>More details about the available preprocessing steps can be found here.</p>"},{"location":"loading_datasets/","title":"Loading Datasets","text":"<p>This section explains how to load and save datasets, what datasets are available and how to register and implement your own data loader.</p>"},{"location":"loading_datasets/#recsysdataset-class","title":"RecSysDataSet Class","text":"<p>The core of the framework's data model is the <code>RecSysDataSet</code> class. This generic class provides a unified interface for handling different types of recommendation system datasets.</p> <p>The <code>RecSysDataSet</code> can contain one of three different variants of data:</p> <ul> <li>RawData: Contains a single pandas DataFrame with all interactions</li> <li>SplitData: Contains train, validation, and test DataFrames </li> <li>FoldedData: Contains multiple folds, each with their own train/validation/test splits</li> </ul> <p>Data Structure</p> <p>All datasets follow a standardized column structure:</p> <ul> <li><code>user</code>: User identifier (normalized to integers starting from 0)</li> <li><code>item</code>: Item identifier (normalized to integers starting from 0)</li> <li><code>rating</code>: Rating value (explicit feedback) or 1 (implicit feedback)</li> <li><code>timestamp</code>: Unix timestamp of the interaction</li> </ul>"},{"location":"loading_datasets/#using-built-in-data-loaders","title":"Using Built-in Data Loaders","text":"<p>The recommended way to load datasets is using the <code>use_dataloader</code> method with registered data loaders:</p> <pre><code>from omnirec import RecSysDataSet\n\n# Load MovieLens 100K dataset\ndataset = RecSysDataSet.use_dataloader(\"MovieLens100K\")\n\n# Force re-download and re-canonicalization\ndataset = RecSysDataSet.use_dataloader(\n    \"MovieLens100K\", \n    force_download=True, \n    force_canonicalize=True\n)\n\n# Specify custom paths\ndataset = RecSysDataSet.use_dataloader(\n    \"MovieLens100K\",\n    raw_dir=\"/path/to/raw/data\",\n    canon_path=\"/path/to/canonicalized/data.csv\"\n)\n</code></pre> <p>The data loading process includes:</p> <ol> <li>Download: Raw data is downloaded if not already present</li> <li>Canonicalization (data is cleaned and standardized): <ul> <li>Duplicate interactions are removed (keeping the latest)</li> <li>User and item identifiers are normalized to consecutive integers</li> <li>Data is saved in a standardized CSV format</li> </ul> </li> </ol>"},{"location":"loading_datasets/#dataset-statistics","title":"Dataset Statistics","text":"<p>You can get basic statistics about loaded datasets:</p> <pre><code># Get number of interactions\nnum_interactions = dataset.num_interactions()\n\n# Get rating range (for explicit feedback datasets)\nmin_rating = dataset.min_rating()\nmax_rating = dataset.max_rating()\n</code></pre>"},{"location":"loading_datasets/#saving-and-loading-datasets","title":"Saving and Loading Datasets","text":"<p>Save any <code>RecSysDataSet</code> object to a <code>.rsds</code> file with the <code>save()</code> function:</p> <pre><code># Save to file (extension .rsds will be added automatically)\ndataset.save(\"my_dataset\")\n\n# Or specify full path\ndataset.save(\"/path/to/my_dataset.rsds\")\n</code></pre> <p>The save format preserves:</p> <ul> <li>All data variants (Raw, Split, or Folded)</li> <li>Metadata about the dataset</li> <li>Version information for compatibility</li> </ul> <p>You can load previously saved datasets with the <code>load()</code> function:</p> <pre><code># Load from .rsds file\ndataset = RecSysDataSet.load(\"my_dataset.rsds\")\n</code></pre>"},{"location":"loading_datasets/#data-variants","title":"Data Variants","text":"<p>RawData</p> <p>Contains all interactions in a single DataFrame:</p> <pre><code># Access the DataFrame\ndf = dataset._data.df\nprint(f\"Dataset has {len(df)} interactions\")\n</code></pre> <p>SplitData</p> <p>Contains separate train, validation, and test sets:</p> <pre><code># Access individual splits\ntrain_df = dataset._data.get(\"train\")\nval_df = dataset._data.get(\"val\")\ntest_df = dataset._data.get(\"test\")\n</code></pre> <p>FoldedData</p> <p>Contains multiple folds for cross-validation:</p> <pre><code># Access folds\nfor fold_idx, split_data in dataset._data.folds.items():\n    print(f\"Fold {fold_idx}:\")\n    print(f\"  Train: {len(split_data.train)} interactions\")\n    print(f\"  Val: {len(split_data.val)} interactions\") \n    print(f\"  Test: {len(split_data.test)} interactions\")\n</code></pre>"},{"location":"loading_datasets/#loading-custom-datasets","title":"Loading Custom Datasets","text":"<p>Creating Custom Data Loaders</p> <p>To implement a custom data loader, create a class that inherits from <code>Loader</code> and implement the <code>info()</code> and <code>load()</code> function:</p> <pre><code>from pathlib import Path\nimport pandas as pd\nfrom omnirec.data_loaders.base import Loader, DatasetInfo\nfrom omnirec.data_loaders.registry import register_dataloader\n\nclass MyCustomLoader(Loader):\n    @staticmethod\n    def info(name: str) -&gt; DatasetInfo:\n        return DatasetInfo(\n            download_urls=\"https://example.com/dataset.zip\",\n            checksum=\"sha256_checksum_here\"  # Optional but recommended\n        )\n\n    @staticmethod\n    def load(source_dir: Path, name: str) -&gt; pd.DataFrame:\n        # Implement your loading logic here\n        # Must return DataFrame with columns: user, item, rating, timestamp\n        df = pd.read_csv(source_dir / \"data.csv\")\n        # Process and return standardized DataFrame\n        return df\n\n# Register the loader\nregister_dataloader(\"MyDataset\", MyCustomLoader)\n\n# Now you can use it\ndataset = RecSysDataSet.use_dataloader(\"MyDataset\")\n</code></pre> <p>Loader Registration</p> <p>You can register loaders under multiple names:</p> <pre><code># Register under multiple names\nregister_dataloader([\"Dataset1\", \"Dataset2\", \"AliasName\"], MyCustomLoader)\n</code></pre> <p>DatasetInfo</p> <p>The <code>DatasetInfo</code> class provides metadata about your dataset:</p> <ul> <li><code>download_urls</code>: URL(s) to download the dataset (string or list of strings)</li> <li><code>checksum</code>: Optional SHA256 checksum for integrity verification</li> </ul> <p>If multiple URLs are provided, they are tried in order until one succeeds.</p>"},{"location":"preprocessing/","title":"Preprocessing Pipeline","text":"<p>This section explains how to preprocess datasets using the framework's preprocessing pipeline. The preprocessing system provides a modular approach to transform datasets through various operations like subsampling, feedback conversion, core pruning, and data splitting.</p> <p>All preprocessing operations inherit from the <code>Preprocessor</code> base class, which defines a common interface for processing datasets. Each preprocessor takes a <code>RecSysDataSet</code> as input and returns a transformed dataset, potentially of a different data variant.</p>"},{"location":"preprocessing/#pipe-class","title":"Pipe Class","text":"<p>The <code>Pipe</code> class allows you to chain multiple preprocessing steps together into a single preprocessing pipeline:</p> <pre><code>from omnirec.preprocess import Pipe, Subsample, MakeImplicit, CorePruning\n\n# Create a preprocessing pipeline\npipeline = Pipe(\n    Subsample(0.1),\n    MakeImplicit(3),\n    CorePruning(5),\n)\n\n# Apply all steps sequentially\nprocessed_dataset = pipeline.process(dataset)\n</code></pre> <p>The pipeline executes each step in the order they were provided, passing the output of one step as input to the next.</p>"},{"location":"preprocessing/#data-processing","title":"Data Processing","text":"<p>Subsample - Reduces the dataset size by sampling a subset of interactions:</p> <pre><code>from omnirec.preprocess import Subsample\n\n# Sample 10% of interactions\nsubsample = Subsample(0.1)\ndataset = subsample.process(dataset)\n\n# Sample exactly 1000 interactions\nsubsample = Subsample(1000)\ndataset = subsample.process(dataset)\n</code></pre> <p>Parameters: - <code>sample_size</code> (int | float): Number or fraction of interactions to sample   - <code>int</code>: Absolute number of interactions   - <code>float</code>: Fraction of dataset (0.0 to 1.0)</p> <p>MakeImplicit - Converts explicit feedback to implicit feedback by filtering interactions above a threshold:</p> <pre><code>from omnirec.preprocess import MakeImplicit\n\n# Keep ratings &gt;= 3\nmake_implicit = MakeImplicit(3)\ndataset = make_implicit.process(dataset)\n\n# Keep top 50% of ratings\nmake_implicit = MakeImplicit(0.5)\ndataset = make_implicit.process(dataset)\n</code></pre> <p>Parameters: - <code>threshold</code> (int | float): Threshold for filtering interactions   - <code>int</code>: Direct rating threshold   - <code>float</code>: Fraction of maximum rating (0.0 to 1.0)</p> <p>CorePruning - Removes users and items with fewer than a specified number of interactions:</p> <pre><code>from omnirec.preprocess import CorePruning\n\n# Keep only users and items with at least 5 interactions\ncore_pruning = CorePruning(5)\ndataset = core_pruning.process(dataset)\n</code></pre> <p>Parameters: - <code>core</code> (int): Minimum number of interactions required for users and items</p>"},{"location":"preprocessing/#data-splitting","title":"Data Splitting","text":"<p>Holdout Splits - Create train/validation/test splits:</p> <pre><code>from omnirec.preprocess import UserHoldout, RandomHoldout\n\n# User-aware split (each user appears in all sets)\nuser_split = UserHoldout(validation_size=0.15, test_size=0.15)\ndataset = user_split.process(dataset)\n\n# Random split (no user constraints)\nrandom_split = RandomHoldout(validation_size=0.15, test_size=0.15)\ndataset = random_split.process(dataset)\n</code></pre> <p>Parameters: - <code>validation_size</code> (float | int): Size of validation set   - <code>float</code>: Proportion of dataset (0.0 to 1.0)   - <code>int</code>: Absolute number of interactions - <code>test_size</code> (float | int): Size of test set   - <code>float</code>: Proportion of dataset (0.0 to 1.0)   - <code>int</code>: Absolute number of interactions</p> <p>Cross-Validation - Create multiple folds for cross-validation:</p> <pre><code>from omnirec.preprocess import UserCrossValidation, RandomCrossValidation\n\n# User-aware cross-validation (each user appears in all splits)\nuser_cv = UserCrossValidation(num_folds=5, validation_size=0.1)\ndataset = user_cv.process(dataset)\n\n# Random cross-validation (no user constraints)\nrandom_cv = RandomCrossValidation(num_folds=5, validation_size=0.1)\ndataset = random_cv.process(dataset)\n</code></pre> <p>Parameters: - <code>num_folds</code> (int): Number of cross-validation folds - <code>validation_size</code> (float | int): Size of validation set in each fold   - <code>float</code>: Proportion of training data (0.0 to 1.0)   - <code>int</code>: Absolute number of interactions</p>"},{"location":"preprocessing/#data-variant-transformation","title":"Data Variant Transformation","text":"<p>The preprocessing operations transform datasets between different data variants:</p> <ul> <li><code>Subsample</code>, <code>MakeImplicit</code>, <code>CorePruning</code>: RawData \u2192 RawData</li> <li><code>UserHoldout</code>, <code>RandomHoldout</code>: RawData \u2192 SplitData  </li> <li><code>UserCrossValidation</code>, <code>RandomCrossValidation</code>: RawData \u2192 FoldedData</li> </ul> <p>Random State - All operations that involve randomness (sampling, splitting) use a consistent random state for reproducibility.</p>"},{"location":"preprocessing/#complete-example","title":"Complete Example","text":"<pre><code>from omnirec import RecSysDataSet\nfrom omnirec.preprocess import (\n    Pipe, Subsample, MakeImplicit, CorePruning, UserCrossValidation\n)\n\n# Load dataset\ndataset = RecSysDataSet.use_dataloader(\"MovieLens100K\")\n\n# Create and apply comprehensive preprocessing pipeline\npipeline = Pipe(\n    Subsample(0.1),                    # Sample 10% of interactions\n    MakeImplicit(3),                   # Convert to implicit (ratings &gt;= 3)\n    CorePruning(5),                    # Keep 5-core users and items\n    UserCrossValidation(5, 0.1)        # 5-fold CV with 10% validation\n)\n\nprocessed_dataset = pipeline.process(dataset)\n\n# Access the cross-validation folds\nfor fold_idx, split_data in processed_dataset._data.folds.items():\n    print(f\"Fold {fold_idx}:\")\n    print(f\"  Train: {len(split_data.train)} interactions\")\n    print(f\"  Validation: {len(split_data.val)} interactions\")\n    print(f\"  Test: {len(split_data.test)} interactions\")\n</code></pre>"}]}