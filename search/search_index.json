{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to OmniRec","text":""},{"location":"#overview","title":"Overview","text":"<p>Welcome to the OmniRec documentation! OmniRec is an open-source Python library designed to be an all-in-one solution for reproducible and interoperable recommender systems experimentation. You can download the full demo paper here.</p> <p>Recommender systems research often faces challenges like fragmented data handling, inconsistent preprocessing, and poor interoperability between different toolkits. These issues can make it difficult to compare results and reproduce studies, slowing down scientific progress. OmniRec tackles these problems by providing a unified, transparent, and easy-to-use workflow for the entire experimentation process.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Massive Dataset Collection: Get standardized access to over 230 datasets right out of the box.</li> <li>Unified Preprocessing: Define your data cleaning, filtering, and splitting pipeline just once and apply it consistently across different models and frameworks.</li> <li>Seamless Integration: OmniRec works with multiple state-of-the-art recommender system frameworks, including RecPack, RecBole, Lenskit, and Elliot.</li> <li>Extensible by Design: Its modular architecture allows you to easily add new datasets, custom preprocessing steps, or interfaces to other frameworks.</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>OmniRec's power comes from its simple yet flexible architecture, which is organized into four interconnected modules. This design enables a smooth end-to-end workflow from loading data to evaluating model performance.</p> <p></p> <p>Click here to download the PDF.</p> <p>The OmniRec architecture, showing its four main components and the experimental workflow.</p> <p>Here\u2019s a brief look at each component:</p>"},{"location":"#1-data-loader","title":"1. Data Loader","text":"<p>The Data Loader is your starting point. It provides a simple way to load any of the registered datasets or even your own custom data. To ensure consistency, it performs initial preparations like removing duplicate interactions and normalizing user and item identifiers. It also exposes key dataset statistics, which are crucial for analysis and reproducible reporting.</p>"},{"location":"#2-preprocessing-pipeline","title":"2. Preprocessing Pipeline","text":"<p>Once your data is loaded, the Preprocessing Pipeline applies all the transformations you need. You can easily perform common operations such as: * Subsampling and k-core filtering * Converting explicit feedback (e.g., ratings) to implicit feedback (e.g., clicks) * Splitting data into training and testing sets using various strategies like random holdout, time-based holdout, or cross-validation.</p> <p>The best part is that this pipeline is completely customizable, so you can add your own functions with minimal effort.</p>"},{"location":"#3-recommender-interface","title":"3. Recommender Interface","text":"<p>This is where the magic of interoperability happens. The Recommender Interface takes your preprocessed data and seamlessly exports it to widely-used recommender frameworks like Lenskit, RecPack, RecBole, and Elliot. This allows you to train models and generate predictions using the specific tools you need, all without having to rewrite your data preparation code. OmniRec even handles the Python environments for each library to solve dependency conflicts.</p>"},{"location":"#4-evaluator","title":"4. Evaluator","text":"<p>Finally, the Evaluator module provides a standardized way to assess your model's performance. It computes common ranking and rating-based metrics like nDCG@k, Recall@k, and RMSE. By centralizing the evaluation logic, OmniRec ensures that results are always comparable, no matter which underlying framework you used to train the model. All results can be stored with complete metadata, guaranteeing traceability and long-term reproducibility.</p>"},{"location":"API_references/","title":"API Reference","text":""},{"location":"API_references/#dataset-management","title":"Dataset Management","text":""},{"location":"API_references/#omnirec.recsys_data_set.RecSysDataSet","title":"<code>omnirec.recsys_data_set.RecSysDataSet(data: Optional[T] = None, meta: _DatasetMeta = _DatasetMeta())</code>","text":"<p>               Bases: <code>Generic[T]</code></p> Source code in <code>src\\omnirec\\recsys_data_set.py</code> <pre><code>def __init__(\n    self, data: Optional[T] = None, meta: _DatasetMeta = _DatasetMeta()\n) -&gt; None:\n    if data:\n        self._data = data\n    self._meta = meta\n</code></pre>"},{"location":"API_references/#omnirec.recsys_data_set.RecSysDataSet.use_dataloader","title":"<code>use_dataloader(data_set: DataSet | str, raw_dir: Optional[PathLike | str] = None, canon_path: Optional[PathLike | str] = None, force_download=False, force_canonicalize=False) -&gt; RecSysDataSet[RawData]</code>  <code>staticmethod</code>","text":"<p>Loads a dataset using a registered DataLoader. If not already done the data set is downloaded and canonicalized. Canonicalization means duplicates are dropped, identifiers are normalized and the data is saved in a standardized format.</p> <p>Parameters:</p> Name Type Description Default <code>data_set</code> <code>DataSet | str</code> <p>The name of the dataset from the DataSet enum. Must be a registered DataLoader name.</p> required <code>raw_dir</code> <code>Optional[PathLike | str]</code> <p>Target directory where the raw data is stored. If not provided, the data is downloaded to the default raw data directory (_DATA_DIR).</p> <code>None</code> <code>canon_path</code> <code>Optional[PathLike | str]</code> <p>Path where the canonicalized data should be saved. If not provided, the data is saved to the default canonicalized data directory (_DATA_DIR / \"canon\").</p> <code>None</code> <code>force_download</code> <code>bool</code> <p>If True, forces re-downloading of the raw data even if it already exists. Defaults to False.</p> <code>False</code> <code>force_canonicalize</code> <code>bool</code> <p>If True, forces re-canonicalization of the data even if a canonicalized file exists. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>RecSysDataSet[RawData]</code> <p>RecSysDataSet[RawData]: The loaded dataset in canonicalized RawData format.</p> Example <pre><code># Load the MovieLens 100K dataset using the registered DataLoader\n# Download the raw data to the default directory and save the canonicalized data to the default path\ndataset = RecSysDataSet.use_dataloader(data_set_name=DataSet.MovieLens100K)\n</code></pre> Source code in <code>src\\omnirec\\recsys_data_set.py</code> <pre><code>@staticmethod\ndef use_dataloader(\n    data_set: DataSet | str,\n    raw_dir: Optional[PathLike | str] = None,  # TODO: Name that right\n    canon_path: Optional[PathLike | str] = None,  # TODO: Name that right\n    force_download=False,\n    force_canonicalize=False,\n) -&gt; \"RecSysDataSet[RawData]\":\n    \"\"\"Loads a dataset using a registered DataLoader. If not already done the data set is downloaded and canonicalized.\n    Canonicalization means duplicates are dropped, identifiers are normalized and the data is saved in a standardized format.\n\n    Args:\n        data_set (DataSet | str): The name of the dataset from the DataSet enum. Must be a registered DataLoader name.\n        raw_dir (Optional[PathLike | str], optional): Target directory where the raw data is stored. If not provided, the data is downloaded to the default raw data directory (_DATA_DIR).\n        canon_path (Optional[PathLike | str], optional): Path where the canonicalized data should be saved. If not provided, the data is saved to the default canonicalized data directory (_DATA_DIR / \"canon\").\n        force_download (bool, optional): If True, forces re-downloading of the raw data even if it already exists. Defaults to False.\n        force_canonicalize (bool, optional): If True, forces re-canonicalization of the data even if a canonicalized file exists. Defaults to False.\n\n    Returns:\n        RecSysDataSet[RawData]: The loaded dataset in canonicalized RawData format.\n\n    Example:\n        ```Python\n        # Load the MovieLens 100K dataset using the registered DataLoader\n        # Download the raw data to the default directory and save the canonicalized data to the default path\n        dataset = RecSysDataSet.use_dataloader(data_set_name=DataSet.MovieLens100K)\n        ```\n    \"\"\"\n    if isinstance(data_set, DataSet):\n        data_set_name = data_set.value\n    else:\n        data_set_name = data_set\n    dataset = RecSysDataSet[RawData]()\n\n    dataset._meta.name = data_set_name\n\n    if canon_path:\n        dataset._meta.canon_pth = Path(canon_path)\n    else:\n        canon_dir = _DATA_DIR / \"canon\"\n        canon_dir.mkdir(parents=True, exist_ok=True)\n        dataset._meta.canon_pth = (canon_dir / data_set_name).with_suffix(\".csv\")\n    if dataset._meta.canon_pth.exists() and not (\n        force_canonicalize or force_download\n    ):\n        logger.info(\n            \"Canonicalized data set already exists, skipping download and canonicalization.\"\n        )\n        dataset._data = RawData(pd.read_csv(dataset._meta.canon_pth))\n        return dataset\n\n    if raw_dir:\n        dataset._meta.raw_dir = Path(raw_dir)\n\n    dataset._data = RawData(\n        registry._run_loader(data_set_name, force_download, dataset._meta.raw_dir)\n    )\n    dataset._canonicalize()\n    return dataset\n</code></pre>"},{"location":"API_references/#omnirec.recsys_data_set.RecSysDataSet.save","title":"<code>save(file: str | PathLike)</code>","text":"<p>Saves the RecSysDataSet object to a file with the default suffix .rsds.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>The path where the file is saved.</p> required Source code in <code>src\\omnirec\\recsys_data_set.py</code> <pre><code>def save(self, file: str | PathLike):\n    \"\"\"Saves the RecSysDataSet object to a file with the default suffix .rsds.\n\n    Args:\n        file (str | PathLike): The path where the file is saved.\n    \"\"\"\n    file = Path(file)\n    if not file.suffix:\n        file = file.with_suffix(\".rsds\")\n    with zipfile.ZipFile(file, \"w\", zipfile.ZIP_STORED) as zf:\n        if isinstance(self._data, RawData):\n            with zf.open(\"data.csv\", \"w\") as data_file:\n                self._data.df.to_csv(data_file, index=False)\n            zf.writestr(\"VARIANT\", \"RawData\")\n        elif isinstance(self._data, SplitData):\n            for filename, data in zip(\n                [\"train\", \"val\", \"test\"],\n                [self._data.train, self._data.val, self._data.test],\n            ):\n                with zf.open(filename + \".csv\", \"w\") as data_file:\n                    data.to_csv(data_file, index=False)\n            zf.writestr(\"VARIANT\", \"SplitData\")\n        elif isinstance(self._data, FoldedData):\n            # TODO: Leveraging the new SplitData.get method this can be simplified:\n            def write_fold(fold: int, split: str, data: pd.DataFrame):\n                with zf.open(f\"{fold}/{split}.csv\", \"w\") as data_file:\n                    data.to_csv(data_file, index=False)\n\n            for fold, splits in self._data.folds.items():\n                write_fold(fold, \"train\", splits.train)\n                write_fold(fold, \"val\", splits.val)\n                write_fold(fold, \"test\", splits.test)\n\n            zf.writestr(\"VARIANT\", \"FoldedData\")\n\n        else:\n            logger.critical(\n                f\"Unknown data variant: {type(self._data).__name__}! Aborting save operation...\"\n            )\n            sys.exit(1)\n\n        zf.writestr(\"META\", json.dumps(asdict(self._meta), default=str))\n        # HACK: Very simple versioning implementation in case we change anything in the future\n        zf.writestr(\"VERSION\", \"1.0.0\")\n</code></pre>"},{"location":"API_references/#omnirec.recsys_data_set.RecSysDataSet.load","title":"<code>load(file: str | PathLike) -&gt; RecSysDataSet[T]</code>  <code>staticmethod</code>","text":"<p>Loads a RecSysDataSet object from a file with the .rsds suffix.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>The path to the .rsds file.</p> required <p>Returns:</p> Type Description <code>RecSysDataSet[T]</code> <p>RecSysDataSet[T]: The loaded RecSysDataSet object.</p> Source code in <code>src\\omnirec\\recsys_data_set.py</code> <pre><code>@staticmethod\ndef load(file: str | PathLike) -&gt; \"RecSysDataSet[T]\":\n    \"\"\"Loads a RecSysDataSet object from a file with the .rsds suffix.\n\n    Args:\n        file (str | PathLike): The path to the .rsds file.\n\n    Returns:\n        RecSysDataSet[T]: The loaded RecSysDataSet object.\n    \"\"\"\n    with zipfile.ZipFile(file, \"r\", zipfile.ZIP_STORED) as zf:\n        version = zf.read(\"VERSION\").decode()\n        # HACK: Very simple versioning implementation in case we change anything in the future\n        if version != \"1.0.0\":\n            logger.critical(f\"Unknown rsds-file version: {version}\")\n            sys.exit(1)\n\n        variant = zf.read(\"VARIANT\").decode()\n\n        if variant == \"RawData\":\n            with zf.open(\"data.csv\", \"r\") as data_file:\n                data = RawData(pd.read_csv(data_file))\n        elif variant == \"SplitData\":\n            dfs: list[pd.DataFrame] = []\n\n            for filename in [\"train\", \"val\", \"test\"]:\n                with zf.open(filename + \".csv\", \"r\") as data_file:\n                    dfs.append(pd.read_csv(data_file))\n\n            data = SplitData(dfs[0], dfs[1], dfs[2])\n        elif variant == \"FoldedData\":\n            folds: dict[int, SplitData] = {}\n\n            for p in zf.namelist():\n                match = RecSysDataSet._folds_file_pattern.match(p)\n                if not match:\n                    continue\n\n                fold = match.group(1)\n                folds.setdefault(\n                    int(fold), SplitData(*[pd.DataFrame() for _ in range(3)])\n                )\n\n            # TODO: Leveraging the new FoldedData.from_split_dict method this can be simplified:\n            def read_fold(fold: int, split: str) -&gt; pd.DataFrame:\n                with zf.open(f\"{fold}/{split}.csv\", \"r\") as data_file:\n                    return pd.read_csv(data_file)\n\n            for fold, split_data in folds.items():\n                split_data.train = read_fold(fold, \"train\")\n                split_data.val = read_fold(fold, \"val\")\n                split_data.test = read_fold(fold, \"test\")\n\n            data = FoldedData(folds)\n        else:\n            logger.critical(\n                f\"Unknown data variant: {variant}! Aborting load operation...\"\n            )\n            sys.exit(1)\n\n        meta = zf.read(\"META\").decode()\n        meta = _DatasetMeta(**json.loads(meta))\n        return cast(RecSysDataSet[T], RecSysDataSet(data, meta))\n</code></pre>"},{"location":"API_references/#data-loaders","title":"Data Loaders","text":""},{"location":"API_references/#omnirec.data_loaders.base.Loader","title":"<code>omnirec.data_loaders.base.Loader</code>","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"API_references/#omnirec.data_loaders.base.Loader.info","title":"<code>info(name: str) -&gt; DatasetInfo</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Provide metadata information about the dataset identified by <code>name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name under which the loader was registered. Different names may return different DatasetInfo implementations depending on the dataset. This is useful when multiple datasets share the same loading logic but have, for example, different download URLs or checksums.</p> required <p>Returns:</p> Name Type Description <code>DatasetInfo</code> <code>DatasetInfo</code> <p>Metadata including download URLs and optional checksum for verification.</p> Source code in <code>src\\omnirec\\data_loaders\\base.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef info(name: str) -&gt; DatasetInfo:\n    \"\"\"Provide metadata information about the dataset identified by `name`.\n\n    Args:\n        name (str): The name under which the loader was registered. Different names may return different DatasetInfo\n            implementations depending on the dataset. This is useful when multiple\n            datasets share the same loading logic but have, for example, different\n            download URLs or checksums.\n\n    Returns:\n        DatasetInfo: Metadata including download URLs and optional checksum for verification.\n    \"\"\"\n</code></pre>"},{"location":"API_references/#omnirec.data_loaders.base.Loader.load","title":"<code>load(source_dir: Path, name: str) -&gt; pd.DataFrame</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Loads dataset from the given directory into a <code>pd.DataFrame</code>. The DataFrame should have the standard columns: - user - item - rating - timestamp</p> <p>Parameters:</p> Name Type Description Default <code>source_dir</code> <code>Path</code> <p>The directory that contains the downloaded dataset files.</p> required <code>name</code> <code>str</code> <p>The name under which the loader was registered. This allows selecting between different datasets that share the same loading logic but differ in structure or file naming.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Loaded dataset as a pd.DataFrame with expected columns.</p> Source code in <code>src\\omnirec\\data_loaders\\base.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef load(source_dir: Path, name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads dataset from the given directory into a `pd.DataFrame`.\n    The DataFrame should have the standard columns:\n    - user\n    - item\n    - rating\n    - timestamp\n\n    Args:\n        source_dir (Path): The directory that contains the downloaded dataset files.\n        name (str): The name under which the loader was registered. This allows selecting between different\n            datasets that share the same loading logic but differ in structure or\n            file naming.\n\n    Returns:\n        pd.DataFrame: Loaded dataset as a pd.DataFrame with expected columns.\n    \"\"\"\n</code></pre>"},{"location":"API_references/#omnirec.data_loaders.base.DatasetInfo","title":"<code>omnirec.data_loaders.base.DatasetInfo(download_urls: Optional[str | list[str]] = None, checksum: Optional[str] = None, download_file_name: Optional[str] = None, verify_tls: bool = True, license_or_registration: bool = False)</code>  <code>dataclass</code>","text":"<p>Metadata about a dataset.</p> <p>Attributes</p> Optional[Union[str, List[str]]] <p>URL or list of URLs to download the dataset. If a list is provided, URLs are tried in order until one succeeds (skipping on checksum mismatch or HTTP errors).</p> Optional[str] <p>Optional SHA256 checksum to verify the downloaded file's integrity. If provided, the downloaded file will be hashed using SHA256 and compared to this value. Use e.g. <code>hashlib.sha256()</code> to compute the checksum in python:</p> <p>download_file_name : Optional[str]     Optional custom file name to use when saving the downloaded dataset.     If not provided, the name will be inferred from the URL. verify_tls : bool     Whether to verify TLS/SSL certificates when downloading.     Defaults is <code>True</code>. license_or_registration : bool     Indicates if the dataset requires a license agreement or registration to access.     Default is <code>False</code>. <pre><code>import hashlib\nhasher = hashlib.sha256()\nwith open(\"ml-100k.zip\", \"rb\") as f:\n    for chunk in iter(lambda: f.read(8192), b\"\"):\n        hasher.update(chunk)\nprint(hasher.hexdigest())\n</code></pre></p>"},{"location":"API_references/#omnirec.data_loaders.registry.register_dataloader","title":"<code>omnirec.data_loaders.registry.register_dataloader(names: str | list[str], cls: type[Loader])</code>","text":"<p>Register a data loader class under one or multiple names.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>str | list[str]</code> <p>Name(s) to register the loader under.</p> required <code>cls</code> <code>type[Loader]</code> <p>Loader class to register. Must inherit from the common <code>Loader</code> base class.</p> required Source code in <code>src\\omnirec\\data_loaders\\registry.py</code> <pre><code>def register_dataloader(names: str | list[str], cls: type[Loader]):\n    \"\"\"Register a data loader class under one or multiple names.\n\n    Args:\n        names (str | list[str]): Name(s) to register the loader under.\n        cls (type[Loader]): Loader class to register. Must inherit from the common `Loader` base class.\n    \"\"\"\n    if type(names) is list:\n        for n in names:\n            _add_loader(n, cls)\n    elif type(names) is str:\n        _add_loader(names, cls)\n</code></pre>"},{"location":"API_references/#omnirec.data_loaders.registry.list_datasets","title":"<code>omnirec.data_loaders.registry.list_datasets() -&gt; list[str]</code>","text":"<p>List all registered dataset names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of all registered dataset names.</p> Source code in <code>src\\omnirec\\data_loaders\\registry.py</code> <pre><code>def list_datasets() -&gt; list[str]:\n    \"\"\"List all registered dataset names.\n\n    Returns:\n        list[str]: A list of all registered dataset names.\n    \"\"\"\n    return list(_DATA_LOADERS.keys())\n</code></pre>"},{"location":"API_references/#preprocessing-pipeline","title":"Preprocessing Pipeline","text":""},{"location":"API_references/#omnirec.preprocess.base.Preprocessor","title":"<code>omnirec.preprocess.base.Preprocessor()</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T, U]</code></p> Source code in <code>src\\omnirec\\preprocess\\base.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n</code></pre>"},{"location":"API_references/#omnirec.preprocess.base.Preprocessor.process","title":"<code>process(dataset: RecSysDataSet[T]) -&gt; RecSysDataSet[U]</code>  <code>abstractmethod</code>","text":"<p>Processes the dataset and returns a new dataset variant.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>RecSysDataSet[T]</code> <p>The dataset to process.</p> required <p>Returns:</p> Type Description <code>RecSysDataSet[U]</code> <p>RecSysDataSet[U]: The processed dataset.</p> Source code in <code>src\\omnirec\\preprocess\\base.py</code> <pre><code>@abstractmethod\ndef process(self, dataset: RecSysDataSet[T]) -&gt; RecSysDataSet[U]:\n    \"\"\"Processes the dataset and returns a new dataset variant.\n\n        Args:\n            dataset (RecSysDataSet[T]): The dataset to process.\n\n        Returns:\n            RecSysDataSet[U]: The processed dataset.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API_references/#omnirec.preprocess.subsample.Subsample","title":"<code>omnirec.preprocess.subsample.Subsample(sample_size: int | float)</code>","text":"<p>               Bases: <code>Preprocessor[RawData, RawData]</code></p> <p>Subsamples the dataset to a specified size.</p> <p>Parameters:</p> Name Type Description Default <code>sample_size</code> <code>int | float</code> <p>The size of the sample to draw from the dataset.                         int: The absolute number of interactions to include in the sample.                         float: The fraction of the dataset to include in the sample (between 0 and 1).</p> required Source code in <code>src\\omnirec\\preprocess\\subsample.py</code> <pre><code>def __init__(self, sample_size: int | float) -&gt; None:\n    \"\"\"Subsamples the dataset to a specified size.\n\n    Args:\n        sample_size (int | float): The size of the sample to draw from the dataset.\n                                    int: The absolute number of interactions to include in the sample.\n                                    float: The fraction of the dataset to include in the sample (between 0 and 1).\n    \"\"\"\n    super().__init__()\n    self.sample_size = sample_size\n</code></pre>"},{"location":"API_references/#omnirec.preprocess.core_pruning.CorePruning","title":"<code>omnirec.preprocess.core_pruning.CorePruning(core: int)</code>","text":"<p>               Bases: <code>Preprocessor[RawData, RawData]</code></p> <p>Prune the dataset to the specified core. Core pruning with a threshold of e.g. 5 means that only users and items with at least 5 interactions are included in the pruned dataset.</p> <p>Parameters:</p> Name Type Description Default <code>core</code> <code>int</code> <p>The core threshold for pruning.</p> required Source code in <code>src\\omnirec\\preprocess\\core_pruning.py</code> <pre><code>def __init__(self, core: int) -&gt; None:\n    \"\"\"Prune the dataset to the specified core.\n    Core pruning with a threshold of e.g. 5 means that only users and items with at least 5 interactions are included in the pruned dataset.\n\n    Args:\n        core (int): The core threshold for pruning.\n    \"\"\"\n    super().__init__()\n    self.core = core\n</code></pre>"},{"location":"API_references/#omnirec.preprocess.feedback_conversion.MakeImplicit","title":"<code>omnirec.preprocess.feedback_conversion.MakeImplicit(threshold: int | float)</code>","text":"<p>               Bases: <code>Preprocessor[RawData, RawData]</code></p> <p>Converts explicit feedback to implicit feedback using the specified threshold.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>int | float</code> <p>The threshold for converting feedback.                         int: Used directly as the threshold, e.g. 3 -&gt; only interactions with a rating of 3 or higher are included.                         float: Interpreted as a fraction of the maximum rating, e.g. 0.5 -&gt; only interactions with a rating of at least 50% of the maximum rating are included.</p> required Source code in <code>src\\omnirec\\preprocess\\feedback_conversion.py</code> <pre><code>def __init__(self, threshold: int | float) -&gt; None:\n    \"\"\"Converts explicit feedback to implicit feedback using the specified threshold.\n\n    Args:\n        threshold (int | float): The threshold for converting feedback.\n                                    int: Used directly as the threshold, e.g. 3 -&gt; only interactions with a rating of 3 or higher are included.\n                                    float: Interpreted as a fraction of the maximum rating, e.g. 0.5 -&gt; only interactions with a rating of at least 50% of the maximum rating are included.\n    \"\"\"\n    super().__init__()\n    self.threshold = threshold\n</code></pre>"},{"location":"API_references/#filtering","title":"Filtering","text":""},{"location":"API_references/#omnirec.preprocess.filter.TimeFilter","title":"<code>omnirec.preprocess.filter.TimeFilter(start: Optional[pd.Timestamp] = None, end: Optional[pd.Timestamp] = None)</code>","text":"<p>               Bases: <code>Preprocessor[RawData, RawData]</code></p> <p>Filters the interactions by a time range. Only interactions within the specified start and end timestamps are retained.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Optional[Timestamp]</code> <p>The start timestamp for the filter. Defaults to None.</p> <code>None</code> <code>end</code> <code>Optional[Timestamp]</code> <p>The end timestamp for the filter. Defaults to None.</p> <code>None</code> Source code in <code>src\\omnirec\\preprocess\\filter.py</code> <pre><code>def __init__(\n    self, start: Optional[pd.Timestamp] = None, end: Optional[pd.Timestamp] = None\n) -&gt; None:\n    \"\"\"Filters the interactions by a time range. Only interactions within the specified start and end timestamps are retained.\n\n    Args:\n        start (Optional[pd.Timestamp], optional): The start timestamp for the filter. Defaults to None.\n        end (Optional[pd.Timestamp], optional): The end timestamp for the filter. Defaults to None.\n    \"\"\"\n    super().__init__()\n    self._start = start\n    self._end = end\n</code></pre>"},{"location":"API_references/#omnirec.preprocess.filter.RatingFilter","title":"<code>omnirec.preprocess.filter.RatingFilter(lower: Optional[int | float] = None, upper: Optional[int | float] = None)</code>","text":"<p>               Bases: <code>Preprocessor[RawData, RawData]</code></p> <p>Filters the interactions by rating values. Only interactions with ratings within the specified lower and upper bounds are retained.</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Optional[int | float]</code> <p>The lower bound for the filter. Defaults to None.</p> <code>None</code> <code>upper</code> <code>Optional[int | float]</code> <p>The upper bound for the filter. Defaults to None.</p> <code>None</code> Source code in <code>src\\omnirec\\preprocess\\filter.py</code> <pre><code>def __init__(\n    self, lower: Optional[int | float] = None, upper: Optional[int | float] = None\n) -&gt; None:\n    \"\"\"Filters the interactions by rating values. Only interactions with ratings within the specified lower and upper bounds are retained.\n\n    Args:\n        lower (Optional[int  |  float], optional): The lower bound for the filter. Defaults to None.\n        upper (Optional[int  |  float], optional): The upper bound for the filter. Defaults to None.\n    \"\"\"\n    super().__init__()\n    self._lower = lower\n    self._upper = upper\n</code></pre>"},{"location":"API_references/#omnirec.preprocess.split.RandomCrossValidation","title":"<code>omnirec.preprocess.split.RandomCrossValidation(num_folds: int, validation_size: float | int)</code>","text":"<p>               Bases: <code>DataSplit[RawData, FoldedData]</code></p> <p>Applies random cross-validation to the dataset. Randomly splits the dataset into training, validation, and test sets for each fold.</p> <p>Parameters:</p> Name Type Description Default <code>num_folds</code> <code>int</code> <p>The number of folds to use for cross-validation.</p> required <code>validation_size</code> <code>float | int</code> <p>float: The proportion (between 0 and 1) of the dataset to include in the validation split.                             int: The absolute number of interactions to include in the validation split.</p> required Source code in <code>src\\omnirec\\preprocess\\split.py</code> <pre><code>def __init__(self, num_folds: int, validation_size: float | int) -&gt; None:\n    \"\"\"Applies random cross-validation to the dataset. Randomly splits the dataset into training, validation, and test sets for each fold.\n\n    Args:\n        num_folds (int): The number of folds to use for cross-validation.\n        validation_size (float | int): float: The proportion (between 0 and 1) of the dataset to include in the validation split.\n                                        int: The absolute number of interactions to include in the validation split.\n    \"\"\"\n    super().__init__(validation_size)\n    self._num_folds = num_folds\n</code></pre>"},{"location":"API_references/#omnirec.preprocess.split.RandomHoldout","title":"<code>omnirec.preprocess.split.RandomHoldout(validation_size: float | int, test_size: float | int)</code>","text":"<p>               Bases: <code>DataSplit[RawData, SplitData]</code></p> <p>Applies a random holdout split to the dataset. Randomly splits the dataset into training, validation, and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>validation_size</code> <code>float | int</code> <p>float: The proportion (between 0 and 1) of the dataset to include in the validation split.                             int: The absolute number of interactions to include in the validation split.</p> required <code>test_size</code> <code>float | int</code> <p>float: The proportion (between 0 and 1) of the dataset to include in the test split.                         int: The absolute number of interactions to include in the test split.</p> required Source code in <code>src\\omnirec\\preprocess\\split.py</code> <pre><code>def __init__(self, validation_size: float | int, test_size: float | int) -&gt; None:\n    \"\"\"Applies a random holdout split to the dataset. Randomly splits the dataset into training, validation, and test sets.\n\n    Args:\n        validation_size (float | int): float: The proportion (between 0 and 1) of the dataset to include in the validation split.\n                                        int: The absolute number of interactions to include in the validation split.\n        test_size (float | int): float: The proportion (between 0 and 1) of the dataset to include in the test split.\n                                    int: The absolute number of interactions to include in the test split.\n    \"\"\"\n    super().__init__(validation_size)\n    self._test_size = test_size\n</code></pre>"},{"location":"API_references/#omnirec.preprocess.split.UserCrossValidation","title":"<code>omnirec.preprocess.split.UserCrossValidation(num_folds: int, validation_size: float | int)</code>","text":"<p>               Bases: <code>DataSplit[RawData, FoldedData]</code></p> <p>Applies user-based cross-validation to the dataset. Ensures that each user has interactions in the training, validation, and test sets in each fold.</p> <p>Parameters:</p> Name Type Description Default <code>num_folds</code> <code>int</code> <p>The number of folds to use for cross-validation.</p> required <code>validation_size</code> <code>float | int</code> <p>float: The proportion (between 0 and 1) of the dataset to include in the validation split.                             int: The absolute number of interactions to include in the validation split.</p> required Source code in <code>src\\omnirec\\preprocess\\split.py</code> <pre><code>def __init__(self, num_folds: int, validation_size: float | int) -&gt; None:\n    \"\"\"Applies user-based cross-validation to the dataset. Ensures that each user has interactions in the training, validation, and test sets in each fold.\n\n    Args:\n        num_folds (int): The number of folds to use for cross-validation.\n        validation_size (float | int): float: The proportion (between 0 and 1) of the dataset to include in the validation split.\n                                        int: The absolute number of interactions to include in the validation split.\n    \"\"\"\n    super().__init__(validation_size)\n    self._num_folds = num_folds\n</code></pre>"},{"location":"API_references/#omnirec.preprocess.split.UserHoldout","title":"<code>omnirec.preprocess.split.UserHoldout(validation_size: float | int, test_size: float | int)</code>","text":"<p>               Bases: <code>DataSplit[RawData, SplitData]</code></p> <p>Applies the user holdout split to the dataset. Ensures that each user has interactions in the training, validation, and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>validation_size</code> <code>float | int</code> <p>float: The proportion (between 0 and 1) of the dataset to include in the validation split.                             int: The absolute number of interactions to include in the validation split.</p> required <code>test_size</code> <code>float | int</code> <p>float: The proportion (between 0 and 1) of the dataset to include in the test split.                         int: The absolute number of interactions to include in the test split.</p> required Source code in <code>src\\omnirec\\preprocess\\split.py</code> <pre><code>def __init__(self, validation_size: float | int, test_size: float | int) -&gt; None:\n    \"\"\"Applies the user holdout split to the dataset. Ensures that each user has interactions in the training, validation, and test sets.\n\n    Args:\n        validation_size (float | int): float: The proportion (between 0 and 1) of the dataset to include in the validation split.\n                                        int: The absolute number of interactions to include in the validation split.\n        test_size (float | int): float: The proportion (between 0 and 1) of the dataset to include in the test split.\n                                    int: The absolute number of interactions to include in the test split.\n    \"\"\"\n    super().__init__(validation_size)\n    self._test_size = test_size\n</code></pre>"},{"location":"API_references/#omnirec.preprocess.split.TimeBasedHoldout","title":"<code>omnirec.preprocess.split.TimeBasedHoldout(validation: float | int | pd.Timestamp, test: float | int | pd.Timestamp)</code>","text":"<p>               Bases: <code>DataSplit[RawData, SplitData]</code></p> <p>Applies a time-based hold-out split on a dataset. Splits the dataset into a train, test and validation split based on the timestamp. Can either use proportions, absolute numbers or timestamps as cut-off criteria.</p> <p>Parameters:</p> Name Type Description Default <code>validation</code> <code>float | int | Timestamp</code> <p>float: The proportion (between 0 and 1) of newest interactions in the dataset to include in the validation split.                                     int: The absolute number of newest interactions to include in the validation split.                                     pd.Timestamp: The timestamp to use as a cut-off for the validation split. Interactions after this timestamp (newer) are included in the validation split.</p> required <code>test</code> <code>float | int | Timestamp</code> <p>float: The proportion (between 0 and 1) of newest interactions in the dataset to include in the test split.                                 int: The absolute number of newest interactions to include in the test split.                                 pd.Timestamp: The timestamp to use as a cut-off for the test split. Interactions after this timestamp (newer) are included in the test split.</p> required Source code in <code>src\\omnirec\\preprocess\\split.py</code> <pre><code>def __init__(\n    self,\n    validation: float | int | pd.Timestamp,\n    test: float | int | pd.Timestamp,\n) -&gt; None:\n    \"\"\"Applies a time-based hold-out split on a dataset. Splits the dataset into a train, test and validation split based on the timestamp. Can either use proportions, absolute numbers or timestamps as cut-off criteria.\n\n    Args:\n        validation (float | int | pd.Timestamp): float: The proportion (between 0 and 1) of newest interactions in the dataset to include in the validation split.\n                                                int: The absolute number of newest interactions to include in the validation split.\n                                                pd.Timestamp: The timestamp to use as a cut-off for the validation split. Interactions after this timestamp (newer) are included in the validation split.\n        test (float | int | pd.Timestamp): float: The proportion (between 0 and 1) of newest interactions in the dataset to include in the test split.\n                                            int: The absolute number of newest interactions to include in the test split.\n                                            pd.Timestamp: The timestamp to use as a cut-off for the test split. Interactions after this timestamp (newer) are included in the test split.\n    \"\"\"\n    super().__init__(0)\n\n    if type(validation) is not type(test):\n        self.logger.critical(\"Validation and test size must be the same type\")\n        sys.exit(1)\n\n    self._valid_size = validation\n    self._test_size = test\n</code></pre>"},{"location":"API_references/#omnirec.preprocess.pipe.Pipe","title":"<code>omnirec.preprocess.pipe.Pipe(*steps: Unpack[tuple[Unpack[Ts], Preprocessor[Any, T]]])</code>","text":"<p>               Bases: <code>Generic[T]</code></p> <p>Pipeline for automatically applying sequential preprocessing steps. Takes as input a sequence of Preprocessor objects. If process() is called, each step's process method is called in the order they were provided. Example:     <pre><code>    # Define preprocessing steps\n    pipe = Pipe(\n        Subsample(0.1),\n        MakeImplicit(3),\n        CorePruning(5),\n        UserCrossValidation(5, 0.1),\n    )\n\n    # Apply the steps\n    dataset = pipe.process(dataset)\n</code></pre></p> Source code in <code>src\\omnirec\\preprocess\\pipe.py</code> <pre><code>def __init__(self, *steps: Unpack[tuple[Unpack[Ts], Preprocessor[Any, T]]]) -&gt; None:\n    \"\"\"Pipeline for automatically applying sequential preprocessing steps. Takes as input a sequence of Preprocessor objects.\n    If process() is called, each step's process method is called in the order they were provided.\n    Example:\n        ```Python\n            # Define preprocessing steps\n            pipe = Pipe(\n                Subsample(0.1),\n                MakeImplicit(3),\n                CorePruning(5),\n                UserCrossValidation(5, 0.1),\n            )\n\n            # Apply the steps\n            dataset = pipe.process(dataset)\n        ```\n    \"\"\"\n    super().__init__()\n    self._steps = steps\n</code></pre>"},{"location":"API_references/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"API_references/#omnirec.runner.evaluation.Evaluator","title":"<code>omnirec.runner.evaluation.Evaluator(*metrics: Metric)</code>","text":"<p>Initialize the Evaluator with metrics to compute on predictions. The Evaluator computes specified metrics on algorithm predictions and accumulates results across experiments. Use <code>get_tables()</code> to retrieve formatted result tables.</p> <p>Parameters:</p> Name Type Description Default <code>*metrics</code> <code>Metric</code> <p>One or more metric instances to compute. Common metrics include NDCG, HR (Hit Rate), and Recall. Each metric can be configured with multiple k values (e.g., <code>NDCG([5, 10, 20])</code>).</p> <code>()</code> Source code in <code>src\\omnirec\\runner\\evaluation.py</code> <pre><code>def __init__(self, *metrics: Metric) -&gt; None:\n    \"\"\"Initialize the Evaluator with metrics to compute on predictions.\n    The Evaluator computes specified metrics on algorithm predictions and accumulates\n    results across experiments. Use `get_tables()` to retrieve formatted result tables.\n\n    Args:\n        *metrics (Metric): One or more metric instances to compute. Common metrics include\n            NDCG, HR (Hit Rate), and Recall. Each metric can be configured with multiple\n            k values (e.g., `NDCG([5, 10, 20])`).\n    \"\"\"\n    if not isinstance(metrics, Iterable):\n        metrics = [metrics]\n    self._metrics = metrics\n    self._results: dict[str, DataFrame] = {}\n</code></pre>"},{"location":"API_references/#metric-base-classes","title":"Metric Base Classes","text":""},{"location":"API_references/#omnirec.metrics.base.Metric","title":"<code>omnirec.metrics.base.Metric</code>","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"API_references/#omnirec.metrics.base.Metric.calculate","title":"<code>omnirec.metrics.base.Metric.calculate(predictions: pd.DataFrame, test: pd.DataFrame) -&gt; MetricResult</code>  <code>abstractmethod</code>","text":"Source code in <code>src\\omnirec\\metrics\\base.py</code> <pre><code>@abstractmethod\ndef calculate(\n    self, predictions: pd.DataFrame, test: pd.DataFrame\n) -&gt; MetricResult: ...\n</code></pre>"},{"location":"API_references/#omnirec.metrics.base.MetricResult","title":"<code>omnirec.metrics.base.MetricResult(name: str, result: float | dict[int, float])</code>  <code>dataclass</code>","text":"<p>Represents the result of a metric calculation. It holds the name as str and either a single float result or a dictionary of results for multiple k values.</p>"},{"location":"API_references/#ranking-metrics","title":"Ranking Metrics","text":""},{"location":"API_references/#omnirec.metrics.ranking.HR","title":"<code>omnirec.metrics.ranking.HR(k: int | list[int])</code>","text":"<p>               Bases: <code>RankingMetric</code></p> <p>Computes the HR metric. k is the number of top recommendations to consider. It can be a single integer or a list of integers, in which case the metric will be computed for each value of k.</p> <p>It follows the formula:</p> <p>\\(HR@k = \\frac{1}{|U|} \\sum_{u \\in U} \\mathbf{1}\\{\\text{Rel}(u) \\cap \\text{Pred}_k(u) \\neq \\emptyset\\}\\)</p> <p>where \\(\\text{Pred}_k(u)\\) is the set of top-k predicted items for user u.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int | list[int]</code> <p>The number of top recommendations to consider.</p> required Source code in <code>src\\omnirec\\metrics\\ranking.py</code> <pre><code>def __init__(self, k: int | list[int]) -&gt; None:\n    \"\"\"\n    Computes the HR metric. k is the number of top recommendations to consider.\n    It can be a single integer or a list of integers, in which case the metric will be computed for each value of k.\n\n    It follows the formula:\n\n    $HR@k = \\\\frac{1}{|U|} \\\\sum_{u \\\\in U} \\\\mathbf{1}\\\\{\\\\text{Rel}(u) \\\\cap \\\\text{Pred}_k(u) \\\\neq \\\\emptyset\\\\}$\n\n    where $\\\\text{Pred}_k(u)$ is the set of top-k predicted items for user u.\n\n    Args:\n        k (int | list[int]): The number of top recommendations to consider.\n    \"\"\"\n    super().__init__(k)\n</code></pre>"},{"location":"API_references/#omnirec.metrics.ranking.HR.calculate","title":"<code>calculate(predictions: DataFrame, test: DataFrame) -&gt; MetricResult</code>","text":"<p>Calculates the Hit Rate (HR) metric. Considers the top-k predictions for one or multiple k values.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>DataFrame</code> <p>Contains the top k predictions for one or more users.</p> required <code>test</code> <code>DataFrame</code> <p>Contains the ground truth relevant items for one or more users.</p> required <p>Returns:</p> Name Type Description <code>MetricResult</code> <code>MetricResult</code> <p>The computed HR scores for each value k. If multiple users are provided, the scores are averaged.</p> Source code in <code>src\\omnirec\\metrics\\ranking.py</code> <pre><code>def calculate(self, predictions: DataFrame, test: DataFrame) -&gt; MetricResult:\n    \"\"\"Calculates the Hit Rate (HR) metric. Considers the top-k predictions for one or multiple k values.\n\n    Args:\n        predictions (DataFrame): Contains the top k predictions for one or more users.\n        test (DataFrame): Contains the ground truth relevant items for one or more users.\n\n    Returns:\n        MetricResult: The computed HR scores for each value k. If multiple users are provided, the scores are averaged.\n    \"\"\"\n    top_k_dict = self.make_topk_dict(predictions)\n\n    hr_per_user_per_k: dict[int, list] = {}\n    # FIXME: Fix metric implementation, adapt to new data format\n    for user, (pred, _) in top_k_dict.items():\n        positive_test_interactions = test[\"item\"][test[\"user\"] == user].to_numpy()\n        hits = np.isin(pred[: max(self._k_list)], positive_test_interactions)\n        for k in self._k_list:\n            user_hr = hits[:k].sum()\n            user_hr = 1 if user_hr &gt; 0 else 0\n            hr_per_user_per_k.setdefault(k, []).append(user_hr)\n    scores: list[float] = [sum(v) / len(v) for v in hr_per_user_per_k.values()]\n    scores_dict = {k: score for k, score in zip(self._k_list, scores)}\n    return MetricResult(__class__.__name__, scores_dict)\n</code></pre>"},{"location":"API_references/#omnirec.metrics.ranking.NDCG","title":"<code>omnirec.metrics.ranking.NDCG(k: int | list[int])</code>","text":"<p>               Bases: <code>RankingMetric</code></p> <p>Initializes the NDCG (Normalized Discounted Cumulative Gain) metric. k is the number of top predictions to consider. It can be a single integer or a list of integers, in which case the metric will be computed for each value of k.</p> <p>The NDCG considers the position of relevant items in a ranked list of predictions.</p> <p>For a user u, the discounted cumulative gain at cutoff k is</p> <p>\\(DCG@k(u) = \\sum_{i=1}^{k} \\frac{\\mathbf{1}\\{\\text{pred}_i \\in \\text{Rel}(u)\\}}{\\log_2(i+1)}\\)</p> <p>where \\(\\mathbf{1}\\{\\cdot\\}\\) is the indicator function and</p> <p>\\(\\text{Rel}(u)\\) is the set of relevant items for user u.</p> <p>The ideal discounted cumulative gain is</p> <p>\\(IDCG@k = \\sum_{i=1}^{k} \\frac{1}{\\log_2(i+1)}\\)</p> <p>The normalized score is</p> <p>\\(NDCG@k(u) = \\frac{DCG@k(u)}{IDCG@k}\\)</p> <p>Finally, the reported score is averaged over all users:</p> <p>\\(\\text{NDCG@k} = \\frac{1}{|U|} \\sum_{u \\in U} NDCG@k(u)\\)</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int | list[int]</code> <p>The number of top predictions to consider.</p> required Source code in <code>src\\omnirec\\metrics\\ranking.py</code> <pre><code>def __init__(self, k: int | list[int]) -&gt; None:\n    \"\"\"Initializes the NDCG (Normalized Discounted Cumulative Gain) metric. k is the number of top predictions to consider.\n    It can be a single integer or a list of integers, in which case the metric will be computed for each value of k.\n\n    The NDCG considers the position of relevant items in a ranked list of predictions.\n\n    For a user u, the discounted cumulative gain at cutoff k is\n\n    $DCG@k(u) = \\\\sum_{i=1}^{k} \\\\frac{\\\\mathbf{1}\\\\{\\\\text{pred}_i \\\\in \\\\text{Rel}(u)\\\\}}{\\\\log_2(i+1)}$\n\n    where $\\\\mathbf{1}\\\\{\\\\cdot\\\\}$ is the indicator function and\n\n    $\\\\text{Rel}(u)$ is the set of relevant items for user u.\n\n    The ideal discounted cumulative gain is\n\n    $IDCG@k = \\\\sum_{i=1}^{k} \\\\frac{1}{\\\\log_2(i+1)}$\n\n    The normalized score is\n\n    $NDCG@k(u) = \\\\frac{DCG@k(u)}{IDCG@k}$\n\n    Finally, the reported score is averaged over all users:\n\n    $\\\\text{NDCG@k} = \\\\frac{1}{|U|} \\\\sum_{u \\\\in U} NDCG@k(u)$\n\n    Args:\n        k (int | list[int]): The number of top predictions to consider.\n    \"\"\"\n    super().__init__(k)\n</code></pre>"},{"location":"API_references/#omnirec.metrics.ranking.NDCG.calculate","title":"<code>calculate(predictions: DataFrame, test: DataFrame) -&gt; MetricResult</code>","text":"<p>Computes the Normalized Discounted Cumulative Gain (NDCG). Considers the top-k predictions for one or multiple k values.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>DataFrame</code> <p>Contains the top k predictions for one or more users.</p> required <code>test</code> <code>DataFrame</code> <p>Contains the ground truth relevant items for one or more users.</p> required <p>Returns:</p> Name Type Description <code>MetricResult</code> <code>MetricResult</code> <p>The computed NDCG scores for each value k. If multiple users are provided, the scores are averaged.</p> Source code in <code>src\\omnirec\\metrics\\ranking.py</code> <pre><code>def calculate(self, predictions: DataFrame, test: DataFrame) -&gt; MetricResult:\n    \"\"\"Computes the Normalized Discounted Cumulative Gain (NDCG). Considers the top-k predictions for one or multiple k values.\n\n    Args:\n        predictions (DataFrame): Contains the top k predictions for one or more users.\n        test (DataFrame): Contains the ground truth relevant items for one or more users.\n\n    Returns:\n        MetricResult: The computed NDCG scores for each value k. If multiple users are provided, the scores are averaged.\n    \"\"\"\n    top_k_dict = self.make_topk_dict(predictions)\n\n    discounted_gain_per_k = np.array(\n        [1 / np.log2(i + 1) for i in range(1, max(self._k_list) + 1)]\n    )\n    ideal_discounted_gain_per_k = [\n        discounted_gain_per_k[: ind + 1].sum()\n        for ind in range(len(discounted_gain_per_k))\n    ]\n    ndcg_per_user_per_k: dict[int, list] = {}\n    for user, (pred, _) in top_k_dict.items():\n        positive_test_interactions = test[\"item\"][test[\"user\"] == user].to_numpy()\n        hits = np.isin(pred[: max(self._k_list)], positive_test_interactions)\n        user_dcg = np.where(hits, discounted_gain_per_k[: len(hits)], 0)\n        for k in self._k_list:\n            user_ndcg = user_dcg[:k].sum() / ideal_discounted_gain_per_k[k - 1]\n            ndcg_per_user_per_k.setdefault(k, []).append(user_ndcg)\n\n    scores: list[float] = [\n        float(sum(v)) / len(v) for v in ndcg_per_user_per_k.values()\n    ]\n    scores_dict = {k: score for k, score in zip(self._k_list, scores)}\n    return MetricResult(__class__.__name__, scores_dict)\n</code></pre>"},{"location":"API_references/#omnirec.metrics.ranking.Recall","title":"<code>omnirec.metrics.ranking.Recall(k: int | list[int])</code>","text":"<p>               Bases: <code>RankingMetric</code></p> <p>Calculates the average recall at k for one or multiple k values. Recall at k is defined as the proportion of relevant items that are found in the top-k recommendations.</p> <p>It follows the formula:</p> <p>\\(Recall@k = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{|\\text{Rel}(u) \\cap \\text{Pred}_k(u)|}{\\min(|\\text{Rel}(u)|, k)}\\)</p> <p>where \\(\\text{Pred}_k(u)\\) is the set of top-k predicted items for user u.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int | list[int]</code> <p>The number of top recommendations to consider.</p> required Source code in <code>src\\omnirec\\metrics\\ranking.py</code> <pre><code>def __init__(self, k: int | list[int]) -&gt; None:\n    \"\"\"Calculates the average recall at k for one or multiple k values. Recall at k is defined as the proportion of relevant items that are found in the top-k recommendations.\n\n    It follows the formula:\n\n    $Recall@k = \\\\frac{1}{|U|} \\\\sum_{u \\\\in U} \\\\frac{|\\\\text{Rel}(u) \\\\cap \\\\text{Pred}_k(u)|}{\\\\min(|\\\\text{Rel}(u)|, k)}$\n\n    where $\\\\text{Pred}_k(u)$ is the set of top-k predicted items for user u.\n\n    Args:\n        k (int | list[int]): The number of top recommendations to consider.\n    \"\"\"\n    super().__init__(k)\n</code></pre>"},{"location":"API_references/#omnirec.metrics.ranking.Recall.calculate","title":"<code>calculate(predictions: DataFrame, test: DataFrame) -&gt; MetricResult</code>","text":"<p>Calculates the Recall metric. Considers the top-k predictions for one or multiple k values.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>DataFrame</code> <p>Contains the top k predictions for one or more users.</p> required <code>test</code> <code>DataFrame</code> <p>Contains the ground truth relevant items for one or more users.</p> required <p>Returns:</p> Type Description <code>MetricResult</code> <p>list[float]: The computed Recall scores for each value k. If multiple users are provided, the scores are averaged.</p> Source code in <code>src\\omnirec\\metrics\\ranking.py</code> <pre><code>def calculate(self, predictions: DataFrame, test: DataFrame) -&gt; MetricResult:\n    \"\"\"Calculates the Recall metric. Considers the top-k predictions for one or multiple k values.\n\n    Args:\n        predictions (DataFrame): Contains the top k predictions for one or more users.\n        test (DataFrame): Contains the ground truth relevant items for one or more users.\n\n    Returns:\n        list[float]: The computed Recall scores for each value k. If multiple users are provided, the scores are averaged.\n    \"\"\"\n    top_k_dict = self.make_topk_dict(predictions)\n\n    recall_per_user_per_k: dict[int, list] = {}\n    for user, (pred, _) in top_k_dict.items():\n        positive_test_interactions = test[\"item\"][test[\"user\"] == user].to_numpy()\n        hits = np.isin(pred[: max(self._k_list)], positive_test_interactions)\n        for k in self._k_list:\n            user_recall = hits[:k].sum() / min(len(positive_test_interactions), k)\n            recall_per_user_per_k.setdefault(k, []).append(user_recall)\n    scores: list[float] = [\n        float(sum(v)) / len(v) for v in recall_per_user_per_k.values()\n    ]\n    scores_dict = {k: score for k, score in zip(self._k_list, scores)}\n    return MetricResult(__class__.__name__, scores_dict)\n</code></pre>"},{"location":"API_references/#prediction-metrics","title":"Prediction Metrics","text":""},{"location":"API_references/#omnirec.metrics.prediction.MAE","title":"<code>omnirec.metrics.prediction.MAE()</code>","text":"<p>               Bases: <code>PredictionMetric</code></p> <p>Mean Absolute Error (MAE) metric. Calculates the average of the absolute differences between predicted and actual ratings, according to the formula: \\(MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\)</p> Source code in <code>src\\omnirec\\metrics\\prediction.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Mean Absolute Error (MAE) metric. Calculates the average of the absolute differences between predicted and actual ratings, according to the formula:\n    $MAE = \\\\frac{1}{n} \\\\sum_{i=1}^{n} |y_i - \\\\hat{y}_i|$\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"API_references/#omnirec.metrics.prediction.MAE.calculate","title":"<code>calculate(predictions: DataFrame, test: DataFrame) -&gt; MetricResult</code>","text":"<p>Calculates the MAE metric.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>DataFrame</code> <p>Contains the predicted ratings.</p> required <code>test</code> <code>DataFrame</code> <p>Contains the ground truth ratings.</p> required <p>Returns:</p> Name Type Description <code>MetricResult</code> <code>MetricResult</code> <p>Contains the name of the metric and the computed MAE value.</p> Source code in <code>src\\omnirec\\metrics\\prediction.py</code> <pre><code>def calculate(self, predictions: DataFrame, test: DataFrame) -&gt; MetricResult:\n    \"\"\"Calculates the MAE metric.\n\n    Args:\n        predictions (DataFrame): Contains the predicted ratings.\n        test (DataFrame): Contains the ground truth ratings.\n\n    Returns:\n        MetricResult: Contains the name of the metric and the computed MAE value.\n    \"\"\"\n    merged = self.merge(predictions, test)\n    mae = mean_absolute_error(merged[\"rating_test\"], merged[\"rating_pred\"])\n    return MetricResult(__class__.__name__, mae)\n</code></pre>"},{"location":"API_references/#omnirec.metrics.prediction.RMSE","title":"<code>omnirec.metrics.prediction.RMSE()</code>","text":"<p>               Bases: <code>PredictionMetric</code></p> <p>Root Mean Squared Error (RMSE) metric. Calculates the square root of the average of the squared differences between predicted and actual ratings, according to the formula:</p> <p>\\(RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\)</p> Source code in <code>src\\omnirec\\metrics\\prediction.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Root Mean Squared Error (RMSE) metric. Calculates the square root of the average of the squared differences between predicted and actual ratings, according to the formula:\n\n    $RMSE = \\\\sqrt{\\\\frac{1}{n} \\\\sum_{i=1}^{n} (y_i - \\\\hat{y}_i)^2}$\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"API_references/#omnirec.metrics.prediction.RMSE.calculate","title":"<code>calculate(predictions: DataFrame, test: DataFrame) -&gt; MetricResult</code>","text":"<p>Calculate the RMSE metric.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>DataFrame</code> <p>description</p> required <code>test</code> <code>DataFrame</code> <p>description</p> required <p>Returns:</p> Name Type Description <code>MetricResult</code> <code>MetricResult</code> <p>Contains the name of the metric and the computed RMSE value.</p> Source code in <code>src\\omnirec\\metrics\\prediction.py</code> <pre><code>def calculate(self, predictions: DataFrame, test: DataFrame) -&gt; MetricResult:\n    \"\"\"Calculate the RMSE metric.\n\n    Args:\n        predictions (DataFrame): _description_\n        test (DataFrame): _description_\n\n    Returns:\n        MetricResult: Contains the name of the metric and the computed RMSE value.\n    \"\"\"\n    merged = self.merge(predictions, test)\n    rmse = root_mean_squared_error(merged[\"rating_test\"], merged[\"rating_pred\"])\n    return MetricResult(__class__.__name__, rmse)\n</code></pre>"},{"location":"API_references/#experiment-planning","title":"Experiment Planning","text":""},{"location":"API_references/#omnirec.runner.plan.ExperimentPlan","title":"<code>omnirec.runner.plan.ExperimentPlan(plan_name: Optional[str] = None)</code>","text":"Source code in <code>src\\omnirec\\runner\\plan.py</code> <pre><code>def __init__(self, plan_name: Optional[str] = None):\n    self._name = plan_name\n    self._config: dict[str, AlgorithmConfig] = {}\n</code></pre>"},{"location":"API_references/#omnirec.runner.plan.ExperimentPlan.add_algorithm","title":"<code>add_algorithm(algorithm: Algorithms | str, algorithm_config: Optional[AlgorithmConfig] = None, force=False)</code>","text":"<p>Adds an algorithm to the experiment plan.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <code>Algorithms | str</code> <p>The algorithm to add.</p> required <code>algorithm_config</code> <code>Optional[AlgorithmConfig]</code> <p>The configuration for the algorithm. Algorithm config depends of the origin library of the algorithm. We refer to their documentation for details about the algorithm hyperparameters.</p> <code>None</code> <code>force</code> <code>bool</code> <p>Whether to forcefully overwrite an existing algorithm config. Defaults to False.</p> <code>False</code> Example <pre><code># Create a new experiment plan\nplan = ExperimentPlan(plan_name=\"Example Plan\")\n\n# Define algorithm configuration based on the lenskit ItemKNNScorer parameters\nlenskit_itemknn = {\"max_nbrs\": [10, 20], \"min_nbrs\": 5, \"feedback\": \"implicit\"}\n\n# Add algorithm with configuration to the plan\nplan.add_algorithm(Algorithms.ItemKNNScorer, lenskit_itemknn)\n</code></pre> Source code in <code>src\\omnirec\\runner\\plan.py</code> <pre><code>def add_algorithm(\n    self,\n    algorithm: Algorithms | str,\n    algorithm_config: Optional[AlgorithmConfig] = None,\n    force=False,\n):\n    \"\"\"Adds an algorithm to the experiment plan.\n\n    Args:\n        algorithm (Algorithms | str): The algorithm to add.\n        algorithm_config (Optional[AlgorithmConfig], optional): The configuration for the algorithm. Algorithm config depends of the origin library of the algorithm. We refer to their documentation for details about the algorithm hyperparameters.\n        force (bool, optional): Whether to forcefully overwrite an existing algorithm config. Defaults to False.\n\n    Example:\n        ```Python\n        # Create a new experiment plan\n        plan = ExperimentPlan(plan_name=\"Example Plan\")\n\n        # Define algorithm configuration based on the lenskit ItemKNNScorer parameters\n        lenskit_itemknn = {\"max_nbrs\": [10, 20], \"min_nbrs\": 5, \"feedback\": \"implicit\"}\n\n        # Add algorithm with configuration to the plan\n        plan.add_algorithm(Algorithms.ItemKNNScorer, lenskit_itemknn)\n        ```    \n    \"\"\"\n    if isinstance(algorithm, Algorithms):\n        algorithm_name = algorithm.value\n    else:\n        algorithm_name = algorithm\n    # TODO: Force option?\n    if not algorithm_config:\n        algorithm_config = {}\n    if algorithm_name in self._config:\n        logger.critical(\n            f'Config for \"{algorithm_name}\" already exists! Use \"force=True\" to overwrite or update it using \"update_algorithm_config()\"'\n        )\n        sys.exit(1)\n\n    self._config[algorithm_name] = algorithm_config\n</code></pre>"},{"location":"API_references/#omnirec.runner.plan.ExperimentPlan.update_algorithm","title":"<code>update_algorithm(algorithm_name: str, algorithm_config: AlgorithmConfig)</code>","text":"<p>Updates the configuration for an existing algorithm in the experiment plan.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_name</code> <code>str</code> <p>The name of the algorithm to update.</p> required <code>algorithm_config</code> <code>AlgorithmConfig</code> <p>The new configuration for the algorithm.</p> required Source code in <code>src\\omnirec\\runner\\plan.py</code> <pre><code>def update_algorithm(self, algorithm_name: str, algorithm_config: AlgorithmConfig):\n    \"\"\"Updates the configuration for an existing algorithm in the experiment plan.\n\n    Args:\n        algorithm_name (str): The name of the algorithm to update.\n        algorithm_config (AlgorithmConfig): The new configuration for the algorithm.\n    \"\"\"\n    if algorithm_name not in self._config:\n        self._config[algorithm_name] = algorithm_config\n    else:\n        self._config[algorithm_name].update(algorithm_config)\n</code></pre>"},{"location":"API_references/#runner-function","title":"Runner Function","text":""},{"location":"API_references/#omnirec.util.run.run_omnirec","title":"<code>omnirec.util.run.run_omnirec(datasets: RecSysDataSet[T] | Iterable[RecSysDataSet[T]], plan: ExperimentPlan, evaluator: Evaluator, slurm_script: Optional[PathLike | str] = None)</code>","text":"<p>Run the OmniRec framework with the specified datasets, experiment plan, and evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>RecSysDataSet[T] | Iterable[RecSysDataSet[T]]</code> <p>The dataset(s) to use for the experiment.</p> required <code>plan</code> <code>ExperimentPlan</code> <p>The experiment plan to follow.</p> required <code>evaluator</code> <code>Evaluator</code> <p>The evaluator to use for the experiment.</p> required <code>slurm_script</code> <code>Optional[PathLike | str]</code> <p>Path to a SLURM script used to schedule experiments on an HPC cluster. If not provided, the experiments are run locally in normal mode.</p> <code>None</code> Source code in <code>src\\omnirec\\util\\run.py</code> <pre><code>def run_omnirec(\n    datasets: RecSysDataSet[T] | Iterable[RecSysDataSet[T]],\n    plan: ExperimentPlan,\n    evaluator: Evaluator,  # TODO: Make optional\n    slurm_script: Optional[PathLike | str] = None\n):\n    \"\"\"Run the OmniRec framework with the specified datasets, experiment plan, and evaluator.\n\n    Args:\n        datasets (RecSysDataSet[T] | Iterable[RecSysDataSet[T]]): The dataset(s) to use for the experiment.\n        plan (ExperimentPlan): The experiment plan to follow.\n        evaluator (Evaluator): The evaluator to use for the experiment.\n        slurm_script (Optional[PathLike | str]): Path to a SLURM script used to schedule experiments\n            on an HPC cluster. If not provided, the experiments are run locally in normal mode.\n    \"\"\"\n    if slurm_script is not None:\n        # TODO:\n        raise NotImplementedError()\n\n    c = Coordinator()\n    c.run(datasets, plan, evaluator)\n\n    for table in evaluator.get_tables():\n        console = Console()\n        console.print(table)\n</code></pre>"},{"location":"API_references/#coordinator-class","title":"Coordinator Class","text":""},{"location":"API_references/#omnirec.runner.coordinator.Coordinator","title":"<code>omnirec.runner.coordinator.Coordinator(checkpoint_dir: PathLike | str = Path('./checkpoints'), tmp_dir: Optional[PathLike | str] = None)</code>","text":"<p>Initialize the Coordinator for orchestrating recommendation algorithm experiments. The Coordinator manages the execution of experiments across multiple datasets, algorithms, and configurations. It handles environment isolation, checkpointing, progress tracking, and communication with framework-specific runners.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>PathLike | str</code> <p>Directory for storing persistent experiment data including model checkpoints, predictions, and progress files. Directory is created if it doesn't exist. Defaults to \"./checkpoints\".</p> <code>Path('./checkpoints')</code> <code>tmp_dir</code> <code>Optional[PathLike | str]</code> <p>Directory for temporary files such as intermediate CSV exports. If None, a temporary directory is created automatically and cleaned up on exit. Defaults to None.</p> <code>None</code> Note <ul> <li>Automatically registers default runners (LensKit, RecBole, RecPack) on initialization</li> <li>Generates SSL certificates for secure RPC communication with runner subprocesses</li> <li>The checkpoint directory structure is: <code>checkpoint_dir/dataset-hash/config-hash/</code></li> </ul> Source code in <code>src\\omnirec\\runner\\coordinator.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_dir: PathLike | str = Path(\"./checkpoints\"),\n    tmp_dir: Optional[PathLike | str] = None,\n) -&gt; None:\n    \"\"\"Initialize the Coordinator for orchestrating recommendation algorithm experiments.\n    The Coordinator manages the execution of experiments across multiple datasets, algorithms,\n    and configurations. It handles environment isolation, checkpointing, progress tracking,\n    and communication with framework-specific runners.\n\n    Args:\n        checkpoint_dir (PathLike | str, optional): Directory for storing persistent experiment data\n            including model checkpoints, predictions, and progress files. Directory is created if it\n            doesn't exist. Defaults to \"./checkpoints\".\n        tmp_dir (Optional[PathLike | str], optional): Directory for temporary files such as intermediate\n            CSV exports. If None, a temporary directory is created automatically and cleaned up on exit.\n            Defaults to None.\n\n    Note:\n        - Automatically registers default runners (LensKit, RecBole, RecPack) on initialization\n        - Generates SSL certificates for secure RPC communication with runner subprocesses\n        - The checkpoint directory structure is: `checkpoint_dir/dataset-hash/config-hash/`\n    \"\"\"\n    self._checkpoint_dir = Path(checkpoint_dir)\n    if tmp_dir:\n        self._tmp_dir = Path(tmp_dir)\n    else:\n        self._tmp_dir_obj: Optional[tempfile.TemporaryDirectory[str]] = (\n            tempfile.TemporaryDirectory()\n        )\n        self._tmp_dir = Path(self._tmp_dir_obj.name)\n\n    self._out_reader: Optional[OutputReader] = None\n    self._err_reader: Optional[OutputReader] = None\n\n    self._register_default_runners()\n    ensure_certs()\n</code></pre>"},{"location":"API_references/#omnirec.runner.coordinator.Coordinator.run","title":"<code>run(datasets: RecSysDataSet[T] | Iterable[RecSysDataSet[T]], config: ExperimentPlan, evaluator: Evaluator) -&gt; Evaluator</code>","text":"<p>Execute recommendation algorithm experiments across datasets and configurations. Orchestrates the complete experiment lifecycle: environment setup, model training, prediction generation, and evaluation. Supports automatic checkpointing and resuming of interrupted experiments.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>RecSysDataSet[T] | Iterable[RecSysDataSet[T]]</code> <p>Single dataset or list of datasets to run experiments on. Datasets must contain either SplitData (train/val/test) or FoldedData (cross-validation folds). Use preprocessing steps to create these splits.</p> required <code>config</code> <code>ExperimentPlan</code> <p>Experiment configuration specifying algorithms and their hyperparameters. Each algorithm in the plan will be executed with all specified parameter combinations.</p> required <code>evaluator</code> <code>Evaluator</code> <p>Evaluator instance containing metrics to compute on predictions. Results are accumulated across all experiments and accessible via <code>evaluator.get_tables()</code>.</p> required <p>Returns:</p> Name Type Description <code>Evaluator</code> <code>Evaluator</code> <p>The same evaluator instance passed in, now containing results from all experiments. Use <code>evaluator.get_tables()</code> to retrieve formatted result tables.</p> <p>Raises:</p> Type Description <code>SystemExit</code> <p>If the experiment plan is empty or if runner/algorithm validation fails.</p> Note <ul> <li>Each algorithm runs in an isolated Python environment with framework-specific dependencies</li> <li>Progress is checkpointed after each phase (Fit, Predict, Eval) for fault tolerance</li> <li>Identical dataset/config combinations are cached and skipped automatically</li> <li>For cross-validation (FoldedData), experiments run sequentially across all folds</li> <li>Runner subprocesses are automatically started and terminated for each algorithm</li> </ul> Source code in <code>src\\omnirec\\runner\\coordinator.py</code> <pre><code>def run(\n    self,\n    datasets: RecSysDataSet[T] | Iterable[RecSysDataSet[T]],\n    config: ExperimentPlan,\n    evaluator: Evaluator,  # TODO: Make optional\n) -&gt; Evaluator:\n    \"\"\"Execute recommendation algorithm experiments across datasets and configurations.\n    Orchestrates the complete experiment lifecycle: environment setup, model training,\n    prediction generation, and evaluation. Supports automatic checkpointing and resuming\n    of interrupted experiments.\n\n    Args:\n        datasets (RecSysDataSet[T] | Iterable[RecSysDataSet[T]]): Single dataset or list of datasets\n            to run experiments on. Datasets must contain either SplitData (train/val/test) or\n            FoldedData (cross-validation folds). Use preprocessing steps to create these splits.\n        config (ExperimentPlan): Experiment configuration specifying algorithms and their hyperparameters.\n            Each algorithm in the plan will be executed with all specified parameter combinations.\n        evaluator (Evaluator): Evaluator instance containing metrics to compute on predictions.\n            Results are accumulated across all experiments and accessible via `evaluator.get_tables()`.\n\n    Returns:\n        Evaluator: The same evaluator instance passed in, now containing results from all experiments.\n            Use `evaluator.get_tables()` to retrieve formatted result tables.\n\n    Raises:\n        SystemExit: If the experiment plan is empty or if runner/algorithm validation fails.\n\n    Note:\n        - Each algorithm runs in an isolated Python environment with framework-specific dependencies\n        - Progress is checkpointed after each phase (Fit, Predict, Eval) for fault tolerance\n        - Identical dataset/config combinations are cached and skipped automatically\n        - For cross-validation (FoldedData), experiments run sequentially across all folds\n        - Runner subprocesses are automatically started and terminated for each algorithm\n    \"\"\"\n    # TODO: Force fit, pred, eval parameters to overwrite status tracker\n    # TODO: Dataset Normalization stuff etc. beforehand\n    exception_occurred = False\n\n    if not isinstance(datasets, Iterable):\n        datasets = [datasets]\n\n    algorithm_configs = config._get_configs()\n    if len(algorithm_configs) == 0:\n        logger.critical(\n            \"Empty configuration. You have to add at least one experiment!\"\n        )\n        sys.exit(1)\n\n    self._evaluator = evaluator\n\n    for current_algo, current_config_list in algorithm_configs:\n        try:\n            host, port = self.start_runner(current_algo)\n\n            logger.info(\"Connecting to runner...\")\n            conn = rpyc.ssl_connect(\n                host,\n                port,\n                get_key_pth(Side.Client),\n                get_cert_pth(Side.Client),\n                config={\"sync_request_timeout\": 600},\n            )\n            root: RunnerService = conn.root\n\n            for current_dataset in datasets:\n                for current_config in current_config_list:\n                    dataset_namehash = f\"{current_dataset._meta.name}-{self.dataset_hash(current_dataset)[:8]}\"\n                    config_namehash = f\"{current_algo}-{self.config_hash(current_algo, current_config)[:8]}\"\n                    current_checkpoint_dir = (\n                        self._checkpoint_dir / dataset_namehash / config_namehash\n                    )\n                    current_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n                    logger.debug(f\"Using checkpoint dir: {current_checkpoint_dir}\")\n\n                    current_tmp_dir = (\n                        self._tmp_dir / dataset_namehash / config_namehash\n                    )\n\n                    current_tmp_dir.mkdir(parents=True, exist_ok=True)\n                    logger.debug(f\"Using tmp dir: {current_tmp_dir}\")\n\n                    progress = RunProgress.load_or_create(\n                        self._checkpoint_dir, (dataset_namehash, config_namehash)\n                    )\n\n                    if isinstance(current_dataset._data, FoldedData):\n\n                        def get_next_fold():\n                            return progress.get_next_fold_or_init(\n                                dataset_namehash, config_namehash\n                            )\n\n                        def reset_phase():\n                            progress.reset_phase(dataset_namehash, config_namehash)\n\n                        def advance_fold():\n                            progress.advance_fold(dataset_namehash, config_namehash)\n\n                        next_fold = get_next_fold()\n\n                        for fold in range(\n                            next_fold, len(current_dataset._data.folds)\n                        ):\n                            fold_data = current_dataset._data.folds[fold]\n\n                            files = self.get_file_paths(\n                                current_checkpoint_dir, current_tmp_dir, fold\n                            )\n                            util.splits_to_csv(files[:3], fold_data)\n\n                            self.run_split(\n                                root,\n                                progress,\n                                current_algo,\n                                current_config,\n                                current_dataset._meta.name,\n                                dataset_namehash,\n                                config_namehash,\n                                *files,\n                                fold,\n                            )\n\n                            advance_fold()\n                            reset_phase()\n                    elif isinstance(current_dataset._data, SplitData):\n                        files = self.get_file_paths(\n                            current_checkpoint_dir, current_tmp_dir\n                        )\n                        util.splits_to_csv(files[:3], current_dataset._data)\n\n                        self.run_split(\n                            root,\n                            progress,\n                            current_algo,\n                            current_config,\n                            current_dataset._meta.name,\n                            dataset_namehash,\n                            config_namehash,\n                            *files,\n                        )\n                    else:\n                        logger.critical(\n                            \"Invalid dataset variant. Dataset has to be either FoldedData or SplitData. Apply a datasplit beforehand\"\n                        )\n                        sys.exit(1)\n\n        except Exception:\n            traceback.print_exc()\n            exception_occurred = True\n        finally:\n            if exception_occurred:\n                self.stop()\n            else:\n                self.stop(logger.info)\n            # print(self._proc.returncode) # TODO: Handle bad return code?\n    return evaluator\n</code></pre>"},{"location":"API_references/#utility-functions","title":"Utility Functions","text":""},{"location":"API_references/#omnirec.util.util.set_random_state","title":"<code>omnirec.util.util.set_random_state(random_state: int) -&gt; None</code>","text":"<p>Set the global random state for reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>int</code> <p>The random state seed.</p> required Source code in <code>src\\omnirec\\util\\util.py</code> <pre><code>def set_random_state(random_state: int) -&gt; None:\n    \"\"\"Set the global random state for reproducibility.\n\n    Args:\n        random_state (int): The random state seed.\n    \"\"\"\n    global _RANDOM_STATE\n    _RANDOM_STATE = random_state\n</code></pre>"},{"location":"API_references/#omnirec.util.util.get_random_state","title":"<code>omnirec.util.util.get_random_state() -&gt; int</code>","text":"<p>Get the global random state for reproducibility.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The current random state seed.</p> Source code in <code>src\\omnirec\\util\\util.py</code> <pre><code>def get_random_state() -&gt; int:\n    \"\"\"Get the global random state for reproducibility.\n\n    Returns:\n        int: The current random state seed.\n    \"\"\"\n    return _RANDOM_STATE\n</code></pre>"},{"location":"about_us/","title":"About Us","text":"<p>We are the Intelligent Systems Group (ISG) at the University of Siegen in Germany, and we developed and maintain OmniRec in collaboration with our colleagues from the University of G\u00f6ttingen.</p> <p>Led by Prof. Dr. Joeran Beel, our group focuses on research in Recommender Systems, Information Retrieval, and Auto-ML. We created OmniRec out of our own need for a robust, standardized experimentation tool, and its earlier versions have served as the basis for many of our research publications.</p> <p>To learn more about our group and our work, please visit our website: isg.beel.org.</p>"},{"location":"algorithms_overview/","title":"Algorithm Overview","text":"<p>The framework integrates algorithms from multiple recommendation system libraries through unified runner interfaces. Use the algorithm identifiers with the <code>ExperimentPlan</code> to configure experiments. Here is the comprehensive list of all available algorithms:</p>"},{"location":"algorithms_overview/#lenskit-algorithms","title":"LensKit Algorithms","text":"<p>LensKit provides traditional collaborative filtering algorithms optimized for research reproducibility.</p> Algorithm Name Type Feedback Description LensKit.PopScorer Baseline Both Popularity-based scorer LensKit.ItemKNNScorer Neighborhood Both Item-based k-nearest neighbors LensKit.UserKNNScorer Neighborhood Both User-based k-nearest neighbors LensKit.ImplicitMFScorer Matrix Factorization Implicit Implicit feedback matrix factorization (ALS) LensKit.BiasedMFScorer Matrix Factorization Explicit Biased matrix factorization LensKit.FunkSVDScorer Matrix Factorization Explicit FunkSVD matrix factorization"},{"location":"algorithms_overview/#recpack-algorithms","title":"RecPack Algorithms","text":"<p>RecPack provides efficient implementations of collaborative filtering algorithms with a focus on scalability and performance.</p> Algorithm Name Type Feedback Description RecPack.SVD Matrix Factorization Explicit Singular Value Decomposition RecPack.NMF Matrix Factorization Explicit Non-negative Matrix Factorization RecPack.ItemKNN Neighborhood Both Item-based k-nearest neighbors"},{"location":"algorithms_overview/#elliot-algorithms","title":"Elliot Algorithms","text":"<p>Elliot provides a comprehensive framework for reproducible recommender systems evaluation with implementations of state-of-the-art algorithms.</p>"},{"location":"algorithms_overview/#baseline-and-neighborhood-methods","title":"Baseline and Neighborhood Methods","text":"Algorithm Name Type Feedback Description Elliot.MostPop Baseline Both Most popular items baseline Elliot.ItemKNN Neighborhood Both Item-based k-nearest neighbors Elliot.UserKNN Neighborhood Both User-based k-nearest neighbors Elliot.SlopeOne Slope One Explicit Slope One collaborative filtering"},{"location":"algorithms_overview/#matrix-factorization","title":"Matrix Factorization","text":"Algorithm Name Type Feedback Description Elliot.AMF Matrix Factorization Explicit Alternating Matrix Factorization Elliot.BPRMF Matrix Factorization Implicit Bayesian Personalized Ranking Matrix Factorization Elliot.BPRMF_batch Matrix Factorization Implicit BPRMF with batch processing Elliot.FM Factorization Machine Explicit Factorization Machine Elliot.FunkSVD Matrix Factorization Explicit FunkSVD matrix factorization Elliot.NonNegMF Matrix Factorization Explicit Non-negative Matrix Factorization Elliot.PureSVD Matrix Factorization Explicit Pure SVD Elliot.SVDpp Matrix Factorization Explicit SVD++ Elliot.WRMF Matrix Factorization Implicit Weighted Regularized Matrix Factorization Elliot.ConvMF Matrix Factorization Explicit Convolutional Matrix Factorization Elliot.DeepFM Deep Learning Explicit Deep Factorization Machine Elliot.DMF Matrix Factorization Explicit Deep Matrix Factorization Elliot.GMF Matrix Factorization Explicit Generalized Matrix Factorization Elliot.NeuMF Deep Learning Implicit Neural Matrix Factorization"},{"location":"algorithms_overview/#autoencoder-based-methods","title":"Autoencoder-Based Methods","text":"Algorithm Name Type Feedback Description Elliot.MultiDAE Deep Learning Implicit Multi-layer Denoising Autoencoder Elliot.MultiVAE Deep Learning Implicit Multi-layer Variational Autoencoder Elliot.ItemAutoRec Deep Learning Explicit Item-based Autoencoder Elliot.UserAutoRec Deep Learning Explicit User-based Autoencoder"},{"location":"algorithms_overview/#graph-based-methods","title":"Graph-Based Methods","text":"Algorithm Name Type Feedback Description Elliot.LightGCN Graph Neural Network Implicit Light Graph Convolutional Network Elliot.NGCF Graph Neural Network Implicit Neural Graph Collaborative Filtering"},{"location":"algorithms_overview/#recbole-algorithms","title":"RecBole Algorithms","text":"<p>RecBole provides a comprehensive collection of modern recommendation algorithms including deep learning and graph-based methods.</p>"},{"location":"algorithms_overview/#baseline-and-neighborhood-methods_1","title":"Baseline and Neighborhood Methods","text":"Algorithm Name Type Feedback Description RecBole.Pop Baseline Both Popularity-based recommender RecBole.ItemKNN Neighborhood Both Item-based k-nearest neighbors RecBole.Random Baseline Both Random recommendation baseline"},{"location":"algorithms_overview/#matrix-factorization_1","title":"Matrix Factorization","text":"Algorithm Name Type Feedback Description RecBole.BPR Matrix Factorization Implicit Bayesian Personalized Ranking RecBole.FISM Matrix Factorization Implicit Factored Item Similarity Models RecBole.NAIS Matrix Factorization Implicit Neural Attentive Item Similarity RecBole.DMF Matrix Factorization Explicit Deep Matrix Factorization RecBole.ENMF Matrix Factorization Implicit Efficient Neural Matrix Factorization RecBole.NNCF Matrix Factorization Implicit Neural Network Collaborative Filtering"},{"location":"algorithms_overview/#deep-learning-methods","title":"Deep Learning Methods","text":"Algorithm Name Type Feedback Description RecBole.NeuMF Deep Learning Implicit Neural Matrix Factorization RecBole.ConvNCF Deep Learning Implicit Convolutional Neural Collaborative Filtering RecBole.CDAE Deep Learning Implicit Collaborative Denoising Auto-Encoder RecBole.MultiVAE Deep Learning Implicit Variational Autoencoders for Collaborative Filtering RecBole.MultiDAE Deep Learning Implicit Denoising Autoencoders for Collaborative Filtering RecBole.MacridVAE Deep Learning Implicit Macroscopic and Microscopic Variational Autoencoder RecBole.RecVAE Deep Learning Implicit Variational Autoencoders for Recommendations RecBole.DiffRec Deep Learning Implicit Diffusion Recommender Model RecBole.LDiffRec Deep Learning Implicit Latent Diffusion Recommender Model"},{"location":"algorithms_overview/#graph-based-methods_1","title":"Graph-Based Methods","text":"Algorithm Name Type Feedback Description RecBole.SpectralCF Graph Neural Network Implicit Spectral Collaborative Filtering RecBole.GCMC Graph Neural Network Explicit Graph Convolutional Matrix Completion RecBole.NGCF Graph Neural Network Implicit Neural Graph Collaborative Filtering RecBole.LightGCN Graph Neural Network Implicit Light Graph Convolutional Network RecBole.DGCF Graph Neural Network Implicit Disentangled Graph Collaborative Filtering RecBole.SGL Graph Neural Network Implicit Self-supervised Graph Learning RecBole.NCL Graph Neural Network Implicit Neighborhood-enriched Contrastive Learning RecBole.LINE Graph Embedding Implicit Large-scale Information Network Embedding"},{"location":"algorithms_overview/#linear-and-optimization-based-methods","title":"Linear and Optimization-Based Methods","text":"Algorithm Name Type Feedback Description RecBole.EASE Linear Implicit Embarrassingly Shallow Autoencoders RecBole.SLIMElastic Linear Implicit Sparse Linear Method with ElasticNet RecBole.ADMMSLIM Linear Implicit ADMM SLIM for Top-N Recommendation RecBole.NCEPLRec Linear Implicit Neighborhood-based Collaborative Filtering with Pairwise Learning RecBole.SimpleX Linear Implicit Simple and Effective Collaborative Filtering"},{"location":"algorithms_overview/#using-algorithms-in-experiments","title":"Using Algorithms in Experiments","text":"<p>Reference algorithms using the format <code>&lt;Runner&gt;.&lt;Algorithm&gt;</code> in your experiment plan:</p> <pre><code>from omnirec.runner.plan import ExperimentPlan\nfrom omnirec.runner.algos import LensKit, RecBole\n\n# Create experiment plan\nplan = ExperimentPlan(\"Algorithm-Comparison\")\n\n# Add LensKit algorithm\nplan.add_algorithm(\n    LensKit.ItemKNNScorer,\n    {\"max_nbrs\": 20, \"min_nbrs\": 5}\n)\n\n# Add RecBole algorithm\nplan.add_algorithm(\n    RecBole.LightGCN,\n    {\"embedding_size\": 64, \"n_layers\": 3}\n)\n</code></pre> <p>See the Algorithm Configuration guide for detailed information on configuring algorithms and hyperparameters.</p>"},{"location":"checkpointing/","title":"Checkpointing and Results","text":"<p>OmniRec automatically saves experiment progress and results to enable fault tolerance and result persistence.</p>"},{"location":"checkpointing/#checkpoint-directory-structure","title":"Checkpoint Directory Structure","text":"<p>The checkpoint directory organizes experiments hierarchically:</p> <pre><code>checkpoints/\n\u251c\u2500\u2500 progress.json                                    # Global progress tracker\n\u251c\u2500\u2500 out.log                                          # Runner stdout logs\n\u251c\u2500\u2500 err.log                                          # Runner stderr logs\n\u2514\u2500\u2500 {dataset-name}-{hash}/                          # Per dataset\n    \u2514\u2500\u2500 {algorithm-name}-{hash}/                    # Per algorithm configuration\n        \u251c\u2500\u2500 predictions.json                        # Model predictions\n        \u251c\u2500\u2500 fold_0/                                 # For cross-validation\n        \u2502   \u2514\u2500\u2500 predictions.json\n        \u251c\u2500\u2500 fold_1/\n        \u2502   \u2514\u2500\u2500 predictions.json\n        \u2514\u2500\u2500 ...\n</code></pre> <p>Key files:</p> <ul> <li><code>progress.json</code>: Tracks experiment phases (Fit, Predict, Eval, Done) for each configuration. Enables resuming interrupted experiments.</li> <li><code>predictions.json</code>: Contains model predictions with columns: <code>user</code>, <code>item</code>, <code>score</code>, <code>rank</code>.</li> <li><code>out.log</code> / <code>err.log</code>: Runner process output for debugging.</li> </ul> <p>Hash-Based Organization</p> <p>The <code>Coordinator</code> generates unique hashes for datasets and configurations:</p> <ul> <li>Dataset hash: Based on the number of interactions, ensuring identical datasets share the same checkpoint directory</li> <li>Configuration hash: Based on algorithm name and hyperparameters, ensuring identical configurations are deduplicated</li> </ul> <p>This hash-based system enables efficient caching and prevents redundant computation.</p>"},{"location":"checkpointing/#resuming-experiments","title":"Resuming Experiments","text":"<p>If an experiment is interrupted, simply run it again with the same configuration:</p> <pre><code>from omnirec.util.run import run_omnirec\n\n# First run - interrupted during training\nrun_omnirec(datasets=dataset, plan=plan, evaluator=evaluator)\n\n# Second run - automatically resumes from last checkpoint\nrun_omnirec(datasets=dataset, plan=plan, evaluator=evaluator)\n</code></pre> <p>The framework automatically: 1. Loads the progress tracker from <code>progress.json</code> 2. Skips completed phases (Fit, Predict, Eval) 3. Continues from the last incomplete phase 4. Reuses existing predictions instead of retraining</p> <p>Progress Phases</p> <p>Each experiment goes through four phases:</p> <ol> <li>Fit: Train the model on training data</li> <li>Predict: Generate predictions on test data</li> <li>Eval: Compute metrics on predictions</li> <li>Done: Experiment complete</li> </ol> <p>The <code>progress.json</code> file tracks the current phase for each experiment configuration. If interrupted, the next run resumes from the last incomplete phase.</p> <p>Cross-Validation Support</p> <p>For cross-validation experiments (FoldedData), progress is tracked per fold:</p> <ul> <li>Each fold goes through all phases independently</li> <li>The progress tracker maintains the current fold number</li> <li>Interrupted experiments resume from the incomplete fold</li> </ul>"},{"location":"checkpointing/#result-format","title":"Result Format","text":"<p>Results are displayed in formatted tables showing:</p> <ul> <li>Algorithm: The algorithm name and configuration hash</li> <li>Fold: Cross-validation fold number (if applicable, hidden if single split)</li> <li>Metrics: All computed metrics at specified k values (e.g., NDCG@10, HR@20)</li> </ul> <p>Example Output</p> <pre><code>MovieLens100K-a3f8e2c1: Evaluation Results\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Algorithm               \u2503 NDCG@10 \u2503 NDCG@20 \u2503 HR@10  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 LensKit.ItemKNN-b7d4a9  \u2502 0.3245  \u2502 0.3891  \u2502 0.6142 \u2502\n\u2502 RecBole.BPR-c8e5f1a3    \u2502 0.3156  \u2502 0.3802  \u2502 0.5987 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>For cross-validation results with multiple folds:</p> <pre><code>MovieLens100K-a3f8e2c1: Evaluation Results\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Algorithm              \u2503 Fold \u2503 NDCG@10  \u2503 HR@10 \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 LensKit.ItemKNN-b7d4a9 \u2502 0   \u2502 0.3201   \u2502 0.6089  \u2502\n\u2502                        \u2502 1   \u2502 0.3289   \u2502 0.6195  \u2502\n\u2502                        \u2502 2   \u2502 0.3245   \u2502 0.6142  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"checkpointing/#caching-and-deduplication","title":"Caching and Deduplication","text":"<p>Experiments are cached based on dataset and configuration hashes. If you run the same experiment multiple times:</p> <ul> <li>The coordinator detects identical configurations via hash comparison</li> <li>Skips redundant computation (all phases marked as Done)</li> <li>Reuses cached predictions and results</li> </ul> <p>This ensures efficient experimentation and prevents accidental duplication of expensive training runs.</p> <p>When Caching Triggers</p> <p>Caching activates when: - Same dataset (same data and number of interactions) - Same algorithm and hyperparameters - Same preprocessing pipeline</p> <p>Even with different variable names or execution contexts, identical experiments are recognized and deduplicated.</p>"},{"location":"checkpointing/#log-files","title":"Log Files","text":"<p>stdout (out.log)</p> <p>Contains standard output from runner processes, including:</p> <ul> <li>Algorithm initialization messages</li> <li>Training progress</li> <li>Model information</li> <li>Framework-specific logs</li> </ul> <p>stderr (err.log)</p> <p>Contains error output from runner processes, including:</p> <ul> <li>Warning messages</li> <li>Error traces</li> <li>Framework warnings</li> <li>Debugging information</li> </ul> <p>Both log files are appended to across multiple experiment runs, providing a complete history of runner activity.</p>"},{"location":"checkpointing/#debugging","title":"Debugging","text":"<p>If experiments fail:</p> <ol> <li>Check <code>err.log</code> for error messages</li> <li>Review <code>out.log</code> for algorithm output</li> <li>Examine <code>progress.json</code> to see which phase failed</li> <li>Verify dataset preprocessing and splitting</li> </ol>"},{"location":"conf_algo/","title":"Algorithm Configuration","text":"<p>This section explains how to configure algorithms and hyperparameters for recommendation experiments. The experiment planning system provides a flexible approach to define algorithm configurations and automatically generate all hyperparameter combinations.</p> <p>The <code>ExperimentPlan</code> class manages algorithm configurations and hyperparameter grids. It expands every combination you define, automating the execution of multiple algorithm variants with different hyperparameter settings.</p>"},{"location":"conf_algo/#experimentplan","title":"ExperimentPlan","text":"<p>The <code>ExperimentPlan</code> class serves as the central configuration for all algorithms in your experiments. Create a plan by optionally providing a name, then add algorithms with their hyperparameters:</p> <pre><code>from omnirec.runner.plan import ExperimentPlan\n\n# Create an experiment plan with optional name\nplan = ExperimentPlan(\"MovieLens-Experiments\")\n</code></pre> <p>The plan name helps organize checkpoints and logs when storing experiment results.</p>"},{"location":"conf_algo/#algorithm-identifiers","title":"Algorithm Identifiers","text":"<p>Algorithms are referenced using the format <code>&lt;Runner&gt;.&lt;Algorithm&gt;</code>. The framework provides enums in <code>omnirec.runner.algos</code> that expose all built-in algorithm names:</p> <pre><code>from omnirec.runner.algos import LensKit, RecBole\n\n# Access algorithm identifiers via enums\nlenskit_algo = LensKit.ImplicitMFScorer  # \"LensKit.ImplicitMFScorer\"\nrecbole_algo = RecBole.LightGCN          # \"RecBole.LightGCN\"\n</code></pre>"},{"location":"conf_algo/#adding-algorithms","title":"Adding Algorithms","text":"<p>Add algorithms to the plan using the <code>add_algorithm</code> method. Provide the algorithm identifier and a dictionary of hyperparameters:</p> <pre><code>from omnirec.runner.algos import LensKit\n\n# Add algorithm with hyperparameters\nplan.add_algorithm(\n    LensKit.ItemKNNScorer,\n    {\n        \"max_nbrs\": 20,        # Fixed value\n        \"min_nbrs\": 5,         # Fixed value\n        \"center\": True         # Fixed value\n    }\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>algorithm</code> (str | Enum): Algorithm identifier in format <code>&lt;Runner&gt;.&lt;Algorithm&gt;</code></li> <li><code>hyperparameters</code> (dict): Dictionary of hyperparameter names and values</li> <li>Single values: Parameter is fixed across all runs</li> <li>Lists: Parameter values for grid search (creates multiple runs)</li> </ul> <p>Hyperparameter Reference:</p> <p>Hyperparameter names and default values are defined by each algorithm's original library implementation. The framework passes your hyperparameter dictionary directly to the underlying algorithm, so refer to the respective library documentation for available parameters and their expected formats.</p>"},{"location":"conf_algo/#hyperparameter-grid-search","title":"Hyperparameter Grid Search","text":"<p>Provide lists of values for any hyperparameter to automatically generate a grid search. The framework creates separate runs for every combination:</p> <pre><code>from omnirec.runner.algos import LensKit\n\n# Grid search with multiple parameter values\nplan.add_algorithm(\n    LensKit.ItemKNNScorer,\n    {\n        \"max_nbrs\": [20, 40],      # Two values\n        \"min_nbrs\": 5,             # Fixed value\n        \"center\": [True, False]    # Two values\n    }\n)\n</code></pre> <p>This configuration generates four separate runs (2 \u00d7 2 combinations):</p> <ul> <li><code>max_nbrs=20, min_nbrs=5, center=True</code></li> <li><code>max_nbrs=20, min_nbrs=5, center=False</code></li> <li><code>max_nbrs=40, min_nbrs=5, center=True</code></li> <li><code>max_nbrs=40, min_nbrs=5, center=False</code></li> </ul>"},{"location":"conf_algo/#multiple-algorithms","title":"Multiple Algorithms","text":"<p>Combine multiple algorithms from different runners in the same experiment plan:</p> <pre><code>from omnirec.runner.algos import LensKit, RecBole\n\n# Add LensKit algorithm\nplan.add_algorithm(\n    LensKit.ItemKNNScorer,\n    {\n        \"max_nbrs\": [20, 40],\n        \"min_nbrs\": 5,\n    }\n)\n\n# Add RecBole algorithm\nplan.add_algorithm(\n    RecBole.LightGCN,\n    {\n        \"learning_rate\": [0.001, 0.005],\n        \"embedding_size\": 64,\n    }\n)\n</code></pre> <p>Each call to <code>add_algorithm</code> appends a new algorithm configuration. Avoid using the same algorithm identifier multiple times unless you intend to overwrite the previous configuration.</p>"},{"location":"conf_algo/#updating-algorithms","title":"Updating Algorithms","text":"<p>Modify existing algorithm configurations using the <code>update_algorithm</code> method:</p> <pre><code># Update previously added algorithm\nplan.update_algorithm(\n    LensKit.ItemKNNScorer,\n    {\n        \"max_nbrs\": [20, 40, 60],  # Add third value\n        \"min_sim\": 0.01            # Add new parameter\n    }\n)\n</code></pre> <p>The update merges with the existing configuration, adding new parameters and updating existing ones.</p>"},{"location":"conf_algo/#running-experiments","title":"Running Experiments","text":"<p>Pass the configured plan to <code>run_omnirec</code> alongside your dataset and evaluator to execute all algorithm configurations:</p> <pre><code>from omnirec import RecSysDataSet\nfrom omnirec.data_loaders.datasets import DataSet\nfrom omnirec.runner.evaluation import Evaluator\nfrom omnirec.metrics.ranking import NDCG, Recall\nfrom omnirec.preprocess.pipe import Pipe\nfrom omnirec.preprocess.feedback_conversion import MakeImplicit\nfrom omnirec.preprocess.split import UserHoldout\nfrom omnirec.util.run import run_omnirec\n\n# Load and preprocess dataset to implicit feedback\ndataset = RecSysDataSet.use_dataloader(DataSet.MovieLens100K)\npipeline = Pipe(\n    MakeImplicit(3),           # Convert to implicit feedback\n    UserHoldout(0.15, 0.15)    # Split data\n)\ndataset = pipeline.process(dataset)\n\n# Configure evaluator with ranking metrics (for implicit feedback)\nevaluator = Evaluator(NDCG([10]), Recall([10]))\n\n# Run all algorithm configurations\nrun_omnirec(dataset, plan, evaluator)\n</code></pre> <p>The framework executes every algorithm configuration in the plan, automatically managing:</p> <ul> <li>Model training and prediction</li> <li>Checkpoint storage under <code>./checkpoints/&lt;dataset&gt;/&lt;algorithm&gt;/</code></li> <li>Metric evaluation via the evaluator</li> <li>Progress tracking and logging</li> </ul>"},{"location":"conf_algo/#multiple-datasets","title":"Multiple Datasets","text":"<p>Run the same experiment plan across multiple datasets by passing a list of datasets:</p> <pre><code># Load multiple datasets\ndatasets = [\n    RecSysDataSet.use_dataloader(DataSet.MovieLens100K),\n    RecSysDataSet.use_dataloader(DataSet.HetrecLastFM),\n]\n\n# Run all algorithms on all datasets\nrun_omnirec(datasets, plan, evaluator)\n</code></pre> <p>Each algorithm configuration in the plan is executed for every dataset. Results are organized separately by dataset in the checkpoint directory structure.</p>"},{"location":"conf_algo/#complete-example","title":"Complete Example","text":"<pre><code>from omnirec import RecSysDataSet\nfrom omnirec.data_loaders.datasets import DataSet\nfrom omnirec.runner.plan import ExperimentPlan\nfrom omnirec.runner.algos import LensKit, RecBole\nfrom omnirec.runner.evaluation import Evaluator\nfrom omnirec.metrics.ranking import NDCG, Recall\nfrom omnirec.preprocess.pipe import Pipe\nfrom omnirec.preprocess.feedback_conversion import MakeImplicit\nfrom omnirec.preprocess.core_pruning import CorePruning\nfrom omnirec.preprocess.split import UserHoldout\nfrom omnirec.util.run import run_omnirec\n\n# Load dataset and preprocess to implicit feedback\ndataset = RecSysDataSet.use_dataloader(DataSet.MovieLens100K)\npipeline = Pipe(\n    MakeImplicit(3),           # Convert to implicit (ratings &gt;= 3)\n    CorePruning(5),            # Keep 5-core users and items\n    UserHoldout(0.15, 0.15)    # Split into train/val/test\n)\ndataset = pipeline.process(dataset)\n\n# Create experiment plan\nplan = ExperimentPlan(\"Comparison-Study\")\n\n# Add multiple algorithms with grid search\nplan.add_algorithm(\n    LensKit.ItemKNNScorer,\n    {\n        \"max_nbrs\": [20, 40],\n        \"min_nbrs\": 5,\n        \"feedback\": \"implicit\"  # Specify implicit feedback mode\n    }\n)\n\nplan.add_algorithm(\n    RecBole.LightGCN,\n    {\n        \"learning_rate\": [0.001, 0.005],\n        \"embedding_size\": [32, 64],\n        \"n_layers\": 3\n    }\n)\n\n# Configure evaluation with ranking metrics (appropriate for implicit feedback)\nevaluator = Evaluator(NDCG([5, 10]), Recall([5, 10]))\n\n# Execute all experiments\nrun_omnirec(dataset, plan, evaluator)\n</code></pre> <p>This executes 6 algorithm runs (2 ItemKNN variants + 4 LightGCN variants) with automatic evaluation of NDCG and Recall metrics. Since LightGCN requires implicit feedback, we preprocess MovieLens100K from explicit ratings to implicit feedback using <code>MakeImplicit(3)</code>, and use ranking metrics appropriate for implicit feedback evaluation.</p>"},{"location":"datasets_overview/","title":"Dataset Overview","text":"<p>The framework includes many built-in datasets. Use the exact name with the <code>use_dataloader</code> function to load a dataset. Here is the comprehensive list with all dataset names:</p> Dataset Name Number of Users Number of Items Number of Ratings Feedback Type AdressaOneWeek 640.503 20.428 2.817.881 implicit AlibabaIFashion 3.569.112 4.463.302 191.394.393 implicit AlibabaMobile 10.000 2.876.947 4.686.904 implicit Amazon2014Books 8.026.324 2.330.066 22.507.155 explicit Amazon2014Electronics 4.201.696 476.002 7.824.482 explicit Amazon2014MoviesAndTv 2.088.620 200.941 4.607.047 explicit Amazon2014CdsAndVinyl 1.578.597 486.360 3.749.004 explicit Amazon2014ClothingShoesAndJewelry 3.117.268 1.136.004 5.748.920 explicit Amazon2014HomeAndKitchen 2.511.610 410.243 4.253.926 explicit Amazon2014KindleStore 1.406.890 430.530 3.205.467 explicit Amazon2014SportsAndOutdoors 1.990.521 478.898 3.268.695 explicit Amazon2014CellPhonesAndAccessories 2.261.045 319.678 3.447.249 explicit Amazon2014HealthAndPersonalCare 1.851.132 252.331 2.982.326 explicit Amazon2014ToysAndGames 1.342.911 327.698 2.252.771 explicit Amazon2014VideoGames 826.767 50.210 1.324.753 explicit Amazon2014ToolsAndHomeImprovement 1.212.468 260.659 1.926.047 explicit Amazon2014Beauty 1.210.271 249.274 2.023.070 explicit Amazon2014AppsForAndroid 1.323.884 61.275 2.638.172 explicit Amazon2014OfficeProducts 909.314 130.006 1.243.186 explicit Amazon2014PetSupplies 740.985 103.288 1.235.316 explicit Amazon2014Automotive 851.418 320.112 1.373.768 explicit Amazon2014GroceryAndGourmetFood 768.438 166.049 1.297.156 explicit Amazon2014PatioLawnAndGarden 714.791 105.984 993.490 explicit Amazon2014Baby 531.890 64.426 915.446 explicit Amazon2014DigitalMusic 478.235 266.414 836.006 explicit Amazon2014MusicalInstruments 339.231 83.046 500.176 explicit Amazon2014AmazonInstantVideo 426.922 23.965 583.933 explicit Amazon2018AmazonFashion 186.189 749.233 875.121 explicit Amazon2018AllBeauty 32.586 324.038 361.605 explicit Amazon2018Appliances 30.252 515.650 590.844 explicit Amazon2018ArtsCraftsAndSewing 302.809 1.579.230 2.733.842 explicit Amazon2018Automotive 925.387 3.873.247 7.815.540 explicit Amazon2018Books 2.930.451 15.362.619 51.062.224 explicit Amazon2018CdsAndVinyl 434.060 1.944.316 4.458.901 explicit Amazon2018CellPhonesAndAccessories 589.534 6.211.701 10.034.040 explicit Amazon2018ClothingShoesAndJewelry 2.681.297 12.483.678 31.663.536 explicit Amazon2018DigitalMusic 456.992 840.372 1.516.551 explicit Amazon2018Electronics 756.489 9.838.676 20.553.480 explicit Amazon2018GiftCards 1.548 128.877 147.136 explicit Amazon2018GroceryAndGourmetFood 283.507 2.695.974 4.889.624 explicit Amazon2018HomeAndKitchen 1.286.050 9.767.606 21.386.322 explicit Amazon2018IndustrialAndScientific 165.764 1.246.131 1.711.995 explicit Amazon2018KindleStore 493.849 2.409.262 5.704.334 explicit Amazon2018LuxuryBeauty 12.120 416.174 535.310 explicit Amazon2018MagazineSubscriptions 2.428 72.098 88.318 explicit Amazon2018MoviesAndTv 182.032 3.826.085 8.506.849 explicit Amazon2018MusicalInstruments 112.222 903.330 1.470.564 explicit Amazon2018OfficeProducts 306.800 3.404.914 5.387.582 explicit Amazon2018PatioLawnAndGarden 276.563 3.097.405 5.053.304 explicit Amazon2018PetSupplies 198.402 3.085.591 6.254.167 explicit Amazon2018PrimePantry 10.814 247.659 447.399 explicit Amazon2018Software 21.663 375.147 450.578 explicit Amazon2018SportsAndOutdoors 957.764 6.703.391 12.601.954 explicit Amazon2018ToolsAndHomeImprovement 559.775 4.704.014 8.730.382 explicit Amazon2018ToysAndGames 624.792 4.204.994 7.998.969 explicit Amazon2018VideoGames 71.982 1.540.618 2.489.395 explicit Amazon2023AllBeauty 632.000 112.600 701.500 explicit Amazon2023AmazonFashion 2.000.000 825.900 2.500.000 explicit Amazon2023Appliances 1.800.000 94.300 2.100.000 explicit Amazon2023ArtsCraftsAndSewing 4.600.000 801.300 9.000.000 explicit Amazon2023Automotive 8.000.000 2.000.000 20.000.000 explicit Amazon2023BabyProducts 3.400.000 217.700 6.000.000 explicit Amazon2023BeautyAndPersonalCare 11.300.000 1.000.000 23.900.000 explicit Amazon2023Books 10.300.000 4.400.000 29.500.000 explicit Amazon2023CdsAndVinyl 1.800.000 701.700 4.800.000 explicit Amazon2023CellPhonesAndAccessories 11.600.000 1.300.000 20.800.000 explicit Amazon2023ClothingShoesAndJewelry 22.600.000 7.200.000 66.000.000 explicit Amazon2023DigitalMusic 101.000 70.500 130.400 explicit Amazon2023Electronics 18.300.000 1.600.000 43.900.000 explicit Amazon2023GiftCards 132.700 1.100 152.400 explicit Amazon2023GroceryAndGourmetFood 7.000.000 603.200 14.300.000 explicit Amazon2023HandmadeProducts 586.600 164.700 664.200 explicit Amazon2023HealthAndHousehold 12.500.000 797.400 25.600.000 explicit Amazon2023HealthAndPersonalCare 461.700 60.300 494.100 explicit Amazon2023HomeAndKitchen 23.200.000 3.700.000 67.400.000 explicit Amazon2023IndustrialAndScientific 3.400.000 427.500 5.200.000 explicit Amazon2023KindleStore 5.600.000 1.600.000 25.600.000 explicit Amazon2023MagazineSubscriptions 60.100 3.400 71.500 explicit Amazon2023MoviesAndTv 6.500.000 747.800 17.300.000 explicit Amazon2023MusicalInstruments 1.800.000 213.600 3.000.000 explicit Amazon2023OfficeProducts 7.600.000 710.400 12.800.000 explicit Amazon2023PatioLawnAndGarden 8.600.000 851.700 16.500.000 explicit Amazon2023PetSupplies 7.800.000 492.700 16.800.000 explicit Amazon2023Software 2.600.000 89.200 4.900.000 explicit Amazon2023SportsAndOutdoors 10.300.000 1.600.000 19.600.000 explicit Amazon2023SubscriptionBoxes 15.200 641 16.200 explicit Amazon2023ToolsAndHomeImprovement 12.200.000 1.500.000 27.000.000 explicit Amazon2023ToysAndGames 8.100.000 890.700 16.300.000 explicit Amazon2023VideoGames 2.800.000 137.200 4.600.000 explicit Amazon2023Unknown 23.100.000 13.200.000 63.800.000 explicit Anime 73.515 11.200 7.813.730 explicit BeerAdvocate 33.388 66.055 1.571.808 explicit RateBeer 29.265 110.369 2.855.232 explicit Behance 63.497 178.788 1.000.000 implicit GoogleLocal2021Alabama 2.077.087 74.600 8.803.325 explicit GoogleLocal2021Alaska 278.695 12.689 1.032.752 explicit GoogleLocal2021Arizona 4.020.106 108.062 18.006.480 explicit GoogleLocal2021Arkansas 1.217.718 47.076 5.013.709 explicit GoogleLocal2021California 14.098.915 513.131 69.285.890 explicit GoogleLocal2021Colorado 3.656.977 106.244 15.345.822 explicit GoogleLocal2021Connecticut 1.419.448 48.936 5.089.012 explicit GoogleLocal2021Delaware 566.738 14.620 1.855.170 explicit GoogleLocal2021DistrictofColumbia 753.560 11.003 1.847.295 explicit GoogleLocal2021Florida 13.832.724 376.192 60.543.660 explicit GoogleLocal2021Georgia 5.620.482 165.395 23.570.208 explicit GoogleLocal2021Hawaii 833.776 21.421 3.037.301 explicit GoogleLocal2021Idaho 968.801 32.983 3.820.976 explicit GoogleLocal2021Illinois 5.373.130 178.203 22.685.631 explicit GoogleLocal2021Indiana 2.848.489 99.900 12.639.193 explicit GoogleLocal2021Iowa 1.159.889 47.445 4.741.915 explicit GoogleLocal2021Kansas 1.359.825 46.036 5.446.105 explicit GoogleLocal2021Kentucky 1.873.160 62.862 7.507.829 explicit GoogleLocal2021Louisiana 1.898.236 62.962 7.382.344 explicit GoogleLocal2021Maine 575.257 24.666 2.169.983 explicit GoogleLocal2021Maryland 2.858.778 77.680 10.513.574 explicit GoogleLocal2021Massachusetts 2.639.379 91.894 10.260.857 explicit GoogleLocal2021Michigan 4.008.458 158.116 20.409.484 explicit GoogleLocal2021Minnesota 2.074.583 80.586 9.353.809 explicit GoogleLocal2021Mississippi 1.056.414 36.932 3.788.144 explicit GoogleLocal2021Missouri 2.982.323 98.939 13.167.690 explicit GoogleLocal2021Montana 526.275 21.533 1.887.673 explicit GoogleLocal2021Nebraska 798.181 29.877 3.219.434 explicit GoogleLocal2021Nevada 2.565.244 48.009 8.683.291 explicit GoogleLocal2021NewHampshire 748.007 24.624 2.602.125 explicit GoogleLocal2021NewJersey 4.213.428 126.572 15.464.057 explicit GoogleLocal2021NewMexico 1.136.149 34.512 4.600.323 explicit GoogleLocal2021NewYork 8.086.412 270.717 32.891.500 explicit GoogleLocal2021NorthCarolina 5.025.701 165.402 21.876.632 explicit GoogleLocal2021NorthDakota 293.523 11.937 1.085.906 explicit GoogleLocal2021Ohio 4.590.857 172.886 22.662.738 explicit GoogleLocal2021Oklahoma 1.865.711 67.704 8.334.288 explicit GoogleLocal2021Oregon 2.764.082 93.006 10.808.294 explicit GoogleLocal2021Pennsylvania 4.957.916 189.836 21.574.012 explicit GoogleLocal2021RhodeIsland 502.479 15.849 1.747.192 explicit GoogleLocal2021SouthCarolina 2.964.113 84.539 11.749.061 explicit GoogleLocal2021SouthDakota 412.911 14.167 1.420.224 explicit GoogleLocal2021Tennessee 3.802.147 110.829 15.638.980 explicit GoogleLocal2021Texas 13.545.569 444.948 65.178.288 explicit GoogleLocal2021Utah 2.210.420 58.538 8.886.755 explicit GoogleLocal2021Vermont 288.211 11.242 832.926 explicit GoogleLocal2021Virginia 4.047.833 119.031 15.627.147 explicit GoogleLocal2021Washington 3.360.838 120.641 16.249.980 explicit GoogleLocal2021WestVirginia 631.093 23.359 2.163.197 explicit GoogleLocal2021Wisconsin 2.257.895 91.522 10.071.305 explicit GoogleLocal2021Wyoming 392.550 12.016 1.112.674 explicit Gowalla 107.092 1.280.969 3.981.334 implicit HetrecLastFM 1.892 12.523 71.064 implicit MovieLens100K 943 1.682 100.000 explicit MovieLens1BSynthetic 2.197.225 855.723 1.226.159.268 implicit MovieLens20M 138.493 26.744 20.000.263 explicit MovieLens25M 162.541 59.047 25.000.095 explicit MovieLensLatest 330.975 83.239 33.832.162 explicit MovieLensLatestSmall 610 9.724 100.836 explicit MovieLens1M 6.040 3.706 1.000.209 explicit MovieLens10M 69.878 10.677 10.000.054 explicit Yelp2018 1.326.101 174.567 5.261.667 explicit Yelp2019 1.637.138 192.606 6.461.396 explicit Yelp2020 1.968.703 209.393 7.735.091 explicit Yelp2021 2.189.457 160.585 8.345.614 explicit Yelp2022 1.987.929 150.346 6.745.760 explicit Yelp2023 1.987.929 150.346 6.745.760 explicit"},{"location":"datasets_overview/#listing-available-datasets","title":"Listing Available Datasets","text":"<p>To see all registered datasets use <code>list_datasets()</code>:</p> <pre><code>from omnirec.data_loaders.registry import list_datasets\n\navailable_datasets = list_datasets()\nprint(\"Available datasets:\", available_datasets)\n</code></pre>"},{"location":"evaluator/","title":"Evaluation Metrics","text":"<p>This section explains how to configure and use metrics to evaluate recommendation algorithms. The evaluation system provides a flexible approach to compute various metrics after model predictions, supporting both explicit and implicit feedback scenarios.</p> <p>The <code>Evaluator</code> class manages metric computation across all algorithm runs. It automatically loads predictions and applies the specified metrics, supporting both holdout and cross-validation splits.</p>"},{"location":"evaluator/#evaluator-class","title":"Evaluator Class","text":"<p>The <code>Evaluator</code> class coordinates metric calculation across experiments. Create an evaluator by passing one or more metric instances, then provide it to <code>run_omnirec</code> to automatically evaluate all algorithm runs:</p> <pre><code>from omnirec.runner.evaluation import Evaluator\nfrom omnirec.metrics.ranking import NDCG, HR, Recall\n\n# Create evaluator with ranking metrics for implicit feedback\nevaluator = Evaluator(\n    NDCG([5, 10, 20]),\n    HR([5, 10, 20]),\n    Recall([5, 10, 20])\n)\n</code></pre> <p>The evaluator can combine multiple metrics of the same type in a single evaluation run. Choose metrics appropriate for your feedback type: use prediction metrics (<code>RMSE</code>, <code>MAE</code>) for explicit ratings, and ranking metrics (<code>NDCG</code>, <code>HR</code>, <code>Recall</code>) for implicit feedback or top-k recommendations.</p>"},{"location":"evaluator/#available-metrics","title":"Available Metrics","text":"<p>Prediction Metrics - For explicit feedback scenarios where ratings are predicted:</p> <pre><code>from omnirec.metrics.prediction import RMSE, MAE\n\n# Root Mean Squared Error\nrmse = RMSE()\n\n# Mean Absolute Error  \nmae = MAE()\n</code></pre> <p>Use prediction metrics when evaluating algorithms that predict rating values, such as matrix factorization methods on explicit feedback datasets.</p> <p>Ranking Metrics - For top-k recommendation scenarios:</p> <pre><code>from omnirec.metrics.ranking import NDCG, HR, Recall\n\n# Normalized Discounted Cumulative Gain at k=[5, 10, 20]\nndcg = NDCG([5, 10, 20])\n\n# Hit Rate at k=[5, 10]\nhr = HR([5, 10])\n\n# Recall at k=[10, 20]\nrecall = Recall([10, 20])\n</code></pre> <p>Ranking metrics evaluate the quality of top-k recommendation lists. Specify cutoff values (k) to measure performance at different list lengths. For example, <code>NDCG([5, 10, 20])</code> computes NDCG@5, NDCG@10, and NDCG@20.</p>"},{"location":"evaluator/#running-experiments-with-evaluation","title":"Running Experiments with Evaluation","text":"<p>Provide the evaluator when launching experiments with <code>run_omnirec</code>. The framework automatically applies all metrics after each algorithm completes:</p> <pre><code>from omnirec import RecSysDataSet\nfrom omnirec.data_loaders.datasets import DataSet\nfrom omnirec.runner.plan import ExperimentPlan\nfrom omnirec.runner.evaluation import Evaluator\nfrom omnirec.runner.algos import RecBole\nfrom omnirec.metrics.ranking import NDCG, Recall\nfrom omnirec.preprocess.pipe import Pipe\nfrom omnirec.preprocess.feedback_conversion import MakeImplicit\nfrom omnirec.preprocess.split import UserHoldout\nfrom omnirec.util.run import run_omnirec\n\n# Load dataset and convert to implicit feedback\ndataset = RecSysDataSet.use_dataloader(DataSet.MovieLens100K)\npipeline = Pipe(\n    MakeImplicit(3),           # Convert ratings &gt;= 3 to implicit feedback\n    UserHoldout(0.15, 0.15)    # Split data\n)\ndataset = pipeline.process(dataset)\n\n# Create experiment plan with implicit feedback algorithm\nplan = ExperimentPlan(\"MyExperiment\")\nplan.add_algorithm(RecBole.BPR)  # BPR is for implicit feedback\n\n# Configure evaluator with ranking metrics (appropriate for implicit feedback)\nevaluator = Evaluator(NDCG([10]), Recall([10]))\n\n# Run experiments with automatic evaluation\nrun_omnirec(dataset, plan, evaluator)\n</code></pre> <p>All metric computations happen automatically without additional code. Ensure your metrics match your data type: ranking metrics (NDCG, HR, Recall) for implicit feedback, and prediction metrics (RMSE, MAE) for explicit feedback.</p> <p>Additionally, metric results are logged during execution and stored in checkpoint directories alongside model predictions.</p>"},{"location":"evaluator/#custom-metrics","title":"Custom Metrics","text":"<p>To implement custom evaluation metrics, create a subclass of <code>omnirec.metrics.base.Metric</code> and implement the <code>calculate</code> method:</p> <pre><code>from omnirec.metrics.base import Metric, MetricResult\nimport pandas as pd\n\nclass CustomMetric(Metric):\n    def calculate(\n        self, \n        predictions: pd.DataFrame, \n        test: pd.DataFrame\n    ) -&gt; MetricResult:\n        \"\"\"\n        Calculate custom metric from predictions and test data.\n\n        Args:\n            predictions: DataFrame with predictions\n            test: DataFrame with ground truth test data\n\n        Returns:\n            MetricResult with metric name and computed value\n        \"\"\"\n        # Implement your metric calculation logic\n        metric_value = self._compute_value(predictions, test)\n\n        return MetricResult(\n            name=\"CustomMetric\",\n            result=metric_value\n        )\n\n    def _compute_value(\n        self, \n        predictions: pd.DataFrame, \n        test: pd.DataFrame\n    ) -&gt; float:\n        # Custom calculation logic\n        pass\n</code></pre> <p>Use custom metrics the same way as built-in metrics:</p> <pre><code>from omnirec.metrics.prediction import RMSE\n\n# Add custom metric to evaluator (both for explicit feedback)\nevaluator = Evaluator(\n    RMSE(),\n    CustomMetric()\n)\n</code></pre> <p>The custom metric will be automatically applied to all algorithm runs alongside the standard metrics. Ensure your custom metric is appropriate for the feedback type of your dataset.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>In this guide we will present how to install OmniRec and to design and implement your first experiments with it.</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>First you need to setup a Python environment with at least Python 3.12 and install the OmniRec library by running:</p> <pre><code>pip install omnirec\n</code></pre>"},{"location":"getting_started/#loading-datasets","title":"Loading Datasets","text":"<p>Central part of the OmniRec library is the <code>RecSysDataSet</code> class. You can load data by calling the static <code>use_dataloader()</code> function that returns a <code>RecSysDataSet</code> object. If provided a registered dataset name with DataSet., <code>use_dataloader()</code> downloads the dataset, removes duplicates and normalizes the identifiers: <pre><code>from omnirec import RecSysDataSet\nfrom omnirec.data_loaders.datasets import DataSet\n\n# Load the MovieLens 100K dataset\ndataset = RecSysDataSet.use_dataloader(DataSet.MovieLens100K)\n</code></pre> <p>We provide a more detailed documentation in Loading Datasets and a list of available datasets in Datasets Overview.</p>"},{"location":"getting_started/#preprocessing-datasets","title":"Preprocessing Datasets","text":"<p>We can now apply the desired preprocessing steps to the data. The easiest way is to define a preprocessing pipeline:</p> <pre><code>from omnirec.preprocess.core_pruning import CorePruning\nfrom omnirec.preprocess.feedback_conversion import MakeImplicit\nfrom omnirec.preprocess.pipe import Pipe\nfrom omnirec.preprocess.split import UserHoldout\n\n# Create a preprocessing pipeline\npipe = Pipe(\n    MakeImplicit(3),           # Convert to implicit feedback (ratings &gt;= 3)\n    CorePruning(5),            # Keep only 5-core users and items\n    UserHoldout(0.15, 0.15)    # Split into train/val/test sets\n)\n\n# Apply all preprocessing steps\ndataset = pipe.process(dataset)\n</code></pre> <p>The <code>pipe.process()</code> function iteratively executes the preprocessing steps.</p> <p>Alternatively, this can be done step by step by creating a single preprocessing step and calling <code>process()</code> on it:</p> <pre><code>from omnirec.preprocess.subsample import Subsample\n\n# Sample 10% of the interactions\nstep = Subsample(sample_size=0.1)\ndataset = step.process(dataset)\n</code></pre> <p>More details about the available preprocessing steps can be found in Preprocessing Pipeline.</p>"},{"location":"getting_started/#configuring-experiments","title":"Configuring Experiments","text":"<p>To run experiments, you need to create an <code>ExperimentPlan</code> that specifies which algorithms to run and their hyperparameters:</p> <pre><code>from omnirec.runner.plan import ExperimentPlan\nfrom omnirec.runner.algos import LensKit, RecBole\n\n# Create a new experiment plan\nplan = ExperimentPlan(plan_name=\"My First Experiment\")\n\n# Add algorithms with configurations\n# For LensKit ItemKNN with different neighborhood sizes\nplan.add_algorithm(\n    LensKit.ItemKNNScorer,\n    {\"max_nbrs\": [10, 20, 30], \"min_nbrs\": 5}\n)\n\n# Add RecBole BPR algorithm with default parameters\nplan.add_algorithm(RecBole.BPR)\n</code></pre> <p>When you provide a list of values for a hyperparameter (like <code>[10, 20, 30]</code> for <code>max_nbrs</code>), the framework will run separate experiments for each value.</p> <p>You can find more information about the available algorithms and how to use them with OmniRec in the Algorithms Overview and Configure Algorithms documentation.</p>"},{"location":"getting_started/#setting-up-evaluation-metrics","title":"Setting Up Evaluation Metrics","text":"<p>Define which metrics to compute on the test set:</p> <p><pre><code>from omnirec import NDCG, HR, Recall\nfrom omnirec.runner.evaluation import Evaluator\n\n# Create evaluator with multiple metrics at different k values\nevaluator = Evaluator(\n    NDCG([5, 10, 20]),      # Normalized Discounted Cumulative Gain\n    HR([5, 10, 20]),        # Hit Rate\n    Recall([5, 10, 20])     # Recall\n)\n</code></pre> You can find more details about the available metrics and how to use them in the Evaluator documentation.</p>"},{"location":"getting_started/#running-experiments","title":"Running Experiments","text":"<p>Now we can run the experiments using the <code>run_omnirec</code> function:</p> <pre><code>from omnirec.util.run import run_omnirec\n\n# Run all experiments\nrun_omnirec(\n    datasets=dataset,\n    plan=plan,\n    evaluator=evaluator\n)\n</code></pre> <p>The <code>run_omnirec</code> function will:</p> <ol> <li>Set up isolated Python environments for each algorithm framework</li> <li>Train each algorithm configuration on the training data</li> <li>Generate predictions on the test data</li> <li>Compute all specified metrics</li> <li>Save checkpoints to allow resuming interrupted experiments</li> </ol> <p>After experiments complete, the results will be printed to the console and can also be accessed through the checkpoint files.</p>"},{"location":"getting_started/#checkpointing-and-results","title":"Checkpointing and Results","text":"<p>OmniRec automatically saves experiment progress and results to the checkpoint directory. If an experiment is interrupted, simply run it again\u2014the <code>run_omnirec</code> function will automatically resume from the last completed phase.</p> <p>For detailed information about checkpoint structure, resuming experiments, and result formats, see the Checkpointing and Results documentation.</p>"},{"location":"getting_started/#reproducibility","title":"Reproducibility","text":"<p>OmniRec uses a global random state to ensure reproducible results across experiments. By default, the random state is set to 42. You can control this using the <code>set_random_state()</code> and <code>get_random_state()</code> functions:</p> <pre><code>from omnirec.util.util import set_random_state, get_random_state\n\n# Set a specific random seed for reproducibility\nset_random_state(123)\n\n# Verify the random state\ncurrent_seed = get_random_state()\nprint(f\"Current random seed: {current_seed}\")  # Output: 123\n</code></pre> <p>Set the random state before running your experiments to ensure consistent results across multiple runs. This affects all random operations in preprocessing (e.g., data splitting) and algorithm training.</p>"},{"location":"getting_started/#complete-example","title":"Complete Example","text":"<p>Here's a complete example that puts it all together:</p> <pre><code>from omnirec import RecSysDataSet\nfrom omnirec.data_loaders.datasets import DataSet\nfrom omnirec.preprocess.pipe import Pipe\nfrom omnirec.preprocess.feedback_conversion import MakeImplicit\nfrom omnirec.preprocess.core_pruning import CorePruning\nfrom omnirec.preprocess.split import UserHoldout\nfrom omnirec.runner.plan import ExperimentPlan\nfrom omnirec.runner.algos import LensKit, RecBole\nfrom omnirec.runner.evaluation import Evaluator\nfrom omnirec.util.run import run_omnirec\n\n# Load and preprocess dataset\ndataset = RecSysDataSet.use_dataloader(DataSet.MovieLens100K)\n\npipeline = Pipe(\n    MakeImplicit(3),\n    CorePruning(5),\n    UserHoldout(0.15, 0.15)\n)\ndataset = pipeline.process(dataset)\n\n# Configure experiments\nplan = ExperimentPlan(plan_name=\"MovieLens Comparison\")\nplan.add_algorithm(LensKit.ItemKNNScorer, {\"max_nbrs\": [20, 30]})\nplan.add_algorithm(RecBole.BPR)\n\n# Set up evaluation\nevaluator = Evaluator(NDCG(10), HR(10))\n\n# Run experiments\nrun_omnirec(datasets=dataset, plan=plan, evaluator=evaluator)\n</code></pre> <p>This example loads MovieLens 100K, preprocesses it, runs ItemKNN and BPR algorithms, evaluates them using NDCG, Hit Rate, and Recall metrics, and displays the results in formatted tables.</p>"},{"location":"loading_datasets/","title":"Loading Datasets","text":"<p>This section explains how to load and save datasets, what datasets are available and how to register and implement your own data loader.</p>"},{"location":"loading_datasets/#recsysdataset-class","title":"RecSysDataSet Class","text":"<p>The core of the framework's data model is the <code>RecSysDataSet</code> class. This generic class provides a unified interface for handling different types of recommendation system datasets.</p> <p>The <code>RecSysDataSet</code> can contain one of three different variants of data:</p> <ul> <li>RawData: Contains a single pandas DataFrame with all interactions</li> <li>SplitData: Contains train, validation, and test DataFrames </li> <li>FoldedData: Contains multiple folds, each with their own train/validation/test splits</li> </ul>"},{"location":"loading_datasets/#data-structure","title":"Data Structure","text":"<p>All datasets follow a standardized column structure:</p> <ul> <li><code>user</code>: User identifier (normalized to integers starting from 0)</li> <li><code>item</code>: Item identifier (normalized to integers starting from 0)</li> <li><code>rating</code>: Rating value (explicit feedback) or 1 (implicit feedback)</li> <li><code>timestamp</code>: Unix timestamp of the interaction</li> </ul>"},{"location":"loading_datasets/#using-built-in-data-loaders","title":"Using Built-in Data Loaders","text":"<p>The recommended way to load datasets is using the <code>use_dataloader</code> method with registered data loaders:</p> <pre><code>from omnirec import RecSysDataSet\nfrom omnirec.data_loaders.datasets import DataSet\n\n# Load MovieLens 100K dataset\ndataset = RecSysDataSet.use_dataloader(DataSet.MovieLens100K)\n\n# Force re-download and re-canonicalization\ndataset = RecSysDataSet.use_dataloader(\n    DataSet.MovieLens100K,\n    force_download=True,\n    force_canonicalize=True\n)\n\n# Specify custom paths\ndataset = RecSysDataSet.use_dataloader(\n    DataSet.MovieLens100K,\n    raw_dir=\"/path/to/raw/data\",\n    canon_path=\"/path/to/canonicalized/data.csv\"\n)\n</code></pre> <p>The data loading process includes: 1. Download: Raw data is downloaded if not already present 2. Canonicalization: Data is cleaned and standardized:    - Duplicate interactions are removed (keeping the latest)    - User and item identifiers are normalized to consecutive integers    - Data is saved in a standardized CSV format</p>"},{"location":"loading_datasets/#dataset-statistics","title":"Dataset Statistics","text":"<p>You can get basic statistics about loaded datasets:</p> <pre><code># Get number of interactions\nnum_interactions = dataset.num_interactions()\n\n# Get rating range (for explicit feedback datasets)\nmin_rating = dataset.min_rating()\nmax_rating = dataset.max_rating()\n</code></pre>"},{"location":"loading_datasets/#saving-and-loading-datasets","title":"Saving and Loading Datasets","text":"<p>Save any <code>RecSysDataSet</code> object to a compressed <code>.rsds</code> file with the <code>save()</code> function:</p> <pre><code># Save to file (extension .rsds will be added automatically)\ndataset.save(\"my_dataset\")\n\n# Or specify full path\ndataset.save(\"/path/to/my_dataset.rsds\")\n</code></pre> <p>The save format preserves: - All data variants (Raw, Split, or Folded) - Metadata about the dataset - Version information for compatibility</p> <p>You can load previously saved datasets with the <code>load()</code> function:</p> <pre><code># Load from .rsds file\ndataset = RecSysDataSet.load(\"my_dataset.rsds\")\n</code></pre>"},{"location":"loading_datasets/#data-variants","title":"Data Variants","text":"<p>RawData</p> <p>Contains all interactions in a single DataFrame:</p> <pre><code># Access the DataFrame\ndf = dataset._data.df\nprint(f\"Dataset has {len(df)} interactions\")\n</code></pre> <p>SplitData</p> <p>Contains separate train, validation, and test sets:</p> <pre><code># Access individual splits\ntrain_df = dataset._data.get(\"train\")\nval_df = dataset._data.get(\"val\")\ntest_df = dataset._data.get(\"test\")\n</code></pre> <p>FoldedData</p> <p>Contains multiple folds for cross-validation:</p> <pre><code># Access folds\nfor fold_idx, split_data in dataset._data.folds.items():\n    print(f\"Fold {fold_idx}:\")\n    print(f\"  Train: {len(split_data.train)} interactions\")\n    print(f\"  Val: {len(split_data.val)} interactions\") \n    print(f\"  Test: {len(split_data.test)} interactions\")\n</code></pre>"},{"location":"loading_datasets/#loading-custom-datasets","title":"Loading Custom Datasets","text":"<p>Creating Custom Data Loaders</p> <p>To implement a custom data loader, create a class that inherits from <code>Loader</code> and implement the <code>info()</code> and <code>load()</code> function:</p> <pre><code>from pathlib import Path\nimport pandas as pd\nfrom omnirec.data_loaders.base import Loader, DatasetInfo\nfrom omnirec.data_loaders.registry import register_dataloader\n\nclass MyCustomLoader(Loader):\n    @staticmethod\n    def info(name: str) -&gt; DatasetInfo:\n        return DatasetInfo(\n            download_urls=\"https://example.com/dataset.zip\",\n            checksum=\"sha256_checksum_here\"  # Optional but recommended\n        )\n\n    @staticmethod\n    def load(source_dir: Path, name: str) -&gt; pd.DataFrame:\n        # Implement your loading logic here\n        # Must return DataFrame with columns: user, item, rating, timestamp\n        df = pd.read_csv(source_dir / \"data.csv\")\n        # Process and return standardized DataFrame\n        return df\n\n# Register the loader\nregister_dataloader(\"MyDataset\", MyCustomLoader)\n\n# Now you can use it\ndataset = RecSysDataSet.use_dataloader(\"MyDataset\")\n</code></pre> <p>Loader Registration</p> <p>You can register loaders under multiple names using <code>register_dataloader()</code>:</p> <pre><code># Register under multiple names\nregister_dataloader([\"Dataset1\", \"Dataset2\", \"AliasName\"], MyCustomLoader)\n</code></pre> <p>DatasetInfo</p> <p>The <code>DatasetInfo</code> class provides metadata about your dataset:</p> <ul> <li><code>download_urls</code>: URL(s) to download the dataset (string or list of strings)</li> <li><code>checksum</code>: Optional SHA256 checksum for integrity verification</li> </ul> <p>If multiple URLs are provided, they are tried in order until one succeeds.</p>"},{"location":"preprocessing/","title":"Preprocessing Pipeline","text":"<p>This section explains how to preprocess datasets using the framework's preprocessing pipeline. The preprocessing system provides a modular approach to transform datasets through various operations like subsampling, feedback conversion, core pruning, and data splitting.</p> <p>All preprocessing operations inherit from the <code>Preprocessor</code> base class, which defines a common interface for processing datasets. Each preprocessor takes a <code>RecSysDataSet</code> as input and returns a transformed dataset, potentially of a different data variant.</p>"},{"location":"preprocessing/#pipe-class","title":"Pipe Class","text":"<p>The <code>Pipe</code> class allows you to chain multiple preprocessing steps together into a single preprocessing pipeline:</p> <pre><code>from omnirec.preprocess import Pipe, Subsample, MakeImplicit, CorePruning\n\n# Create a preprocessing pipeline\npipeline = Pipe(\n    Subsample(0.1),\n    MakeImplicit(3),\n    CorePruning(5),\n)\n\n# Apply all steps sequentially\nprocessed_dataset = pipeline.process(dataset)\n</code></pre> <p>The pipeline executes each step in the order they were provided, passing the output of one step as input to the next.</p>"},{"location":"preprocessing/#data-processing","title":"Data Processing","text":"<p>Subsample - Reduces the dataset size by sampling a subset of interactions:</p> <pre><code>from omnirec.preprocess import Subsample\n\n# Sample 10% of interactions\nsubsample = Subsample(0.1)\ndataset = subsample.process(dataset)\n\n# Sample exactly 1000 interactions\nsubsample = Subsample(1000)\ndataset = subsample.process(dataset)\n</code></pre> <p>Parameters:</p> <ul> <li><code>sample_size</code> (int | float): Number or fraction of interactions to sample<ul> <li><code>int</code>: Absolute number of interactions</li> <li><code>float</code>: Fraction of dataset (0.0 to 1.0)</li> </ul> </li> </ul> <p>MakeImplicit - Converts explicit feedback to implicit feedback by filtering interactions above a threshold:</p> <pre><code>from omnirec.preprocess import MakeImplicit\n\n# Keep ratings &gt;= 3\nmake_implicit = MakeImplicit(3)\ndataset = make_implicit.process(dataset)\n\n# Keep top 50% of ratings\nmake_implicit = MakeImplicit(0.5)\ndataset = make_implicit.process(dataset)\n</code></pre> <p>Parameters:</p> <ul> <li><code>threshold</code> (int | float): Threshold for filtering interactions<ul> <li><code>int</code>: Direct rating threshold</li> <li><code>float</code>: Fraction of maximum rating (0.0 to 1.0)</li> </ul> </li> </ul> <p>CorePruning - Removes users and items with fewer than a specified number of interactions:</p> <pre><code>from omnirec.preprocess import CorePruning\n\n# Keep only users and items with at least 5 interactions\ncore_pruning = CorePruning(5)\ndataset = core_pruning.process(dataset)\n</code></pre> <p>Parameters:</p> <ul> <li><code>core</code> (int): Minimum number of interactions required for users and items</li> </ul>"},{"location":"preprocessing/#filtering","title":"Filtering","text":"<p>Apply filtering based on timestamps or ratings to the dataset:</p> <p>TimeFilter - Filters interactions based on timestamp range:</p> <pre><code>import pandas as pd\nfrom omnirec.preprocess.filter import TimeFilter\n\n# Keep interactions from January 1998\ntime_filter = TimeFilter(\n    start=pd.Timestamp(year=1998, month=1, day=1),\n    end=pd.Timestamp(year=1998, month=1, day=30),\n)\ndataset = time_filter.process(dataset)\n\n# Keep interactions after a specific date (no end date)\ntime_filter = TimeFilter(start=pd.Timestamp(year=1998, month=3, day=1))\ndataset = time_filter.process(dataset)\n</code></pre> <p>Parameters:</p> <ul> <li><code>start</code> (Optional[pd.Timestamp]): Start timestamp for the filter (inclusive)</li> <li><code>end</code> (Optional[pd.Timestamp]): End timestamp for the filter (inclusive)</li> </ul> <p>RatingFilter - Filters interactions based on rating values:</p> <pre><code>from omnirec.preprocess.filter import RatingFilter\n\n# Keep ratings between 1 and 3\nrating_filter = RatingFilter(lower=1, upper=3)\ndataset = rating_filter.process(dataset)\n\n# Keep ratings &gt;= 4 (no upper bound)\nrating_filter = RatingFilter(lower=4)\ndataset = rating_filter.process(dataset)\n</code></pre> <p>Parameters:</p> <ul> <li><code>lower</code> (Optional[int | float]): Lower bound for rating values (inclusive)</li> <li><code>upper</code> (Optional[int | float]): Upper bound for rating values (inclusive)</li> </ul>"},{"location":"preprocessing/#data-splitting","title":"Data Splitting","text":""},{"location":"preprocessing/#holdout-splits","title":"Holdout Splits","text":"<p>Create train/validation/test splits:</p> <pre><code>from omnirec.preprocess import UserHoldout, RandomHoldout\n\n# User-aware split (each user appears in all sets)\nuser_split = UserHoldout(validation_size=0.15, test_size=0.15)\ndataset = user_split.process(dataset)\n\n# Random split (no user constraints)\nrandom_split = RandomHoldout(validation_size=0.15, test_size=0.15)\ndataset = random_split.process(dataset)\n</code></pre> <p>UserHoldout ensures that each user has interactions in all splits, while RandomHoldout randomly assigns interactions without user constraints.</p> <p>Parameters:</p> <ul> <li><code>validation_size</code> (float | int): Size of validation set<ul> <li><code>float</code>: Proportion of dataset (0.0 to 1.0)</li> <li><code>int</code>: Absolute number of interactions</li> </ul> </li> <li><code>test_size</code> (float | int): Size of test set<ul> <li><code>float</code>: Proportion of dataset (0.0 to 1.0)</li> <li><code>int</code>: Absolute number of interactions</li> </ul> </li> </ul>"},{"location":"preprocessing/#cross-validation","title":"Cross-Validation","text":"<p>Create multiple folds for cross-validation:</p> <pre><code>from omnirec.preprocess import UserCrossValidation, RandomCrossValidation\n\n# User-aware cross-validation (each user appears in all splits)\nuser_cv = UserCrossValidation(num_folds=5, validation_size=0.2)\ndataset = user_cv.process(dataset)\n\n# Random cross-validation (no user constraints)\nrandom_cv = RandomCrossValidation(num_folds=5, validation_size=0.2)\ndataset = random_cv.process(dataset)\n</code></pre> <p>Cross-Validation creates multiple folds for evaluation, so different interactions are used for validation in each fold. UserCrossValidation ensures that each user has interactions in all splits, while RandomCrossValidation randomly assigns interactions without user constraints.</p> <p>Parameters:</p> <ul> <li><code>num_folds</code> (int): Number of cross-validation folds</li> <li><code>validation_size</code> (float | int): Size of validation set in each fold<ul> <li><code>float</code>: Proportion of training data (0.0 to 1.0)</li> <li><code>int</code>: Absolute number of interactions</li> </ul> </li> </ul>"},{"location":"preprocessing/#time-based-splits","title":"Time-based Splits","text":"<p>Create splits based on timestamps.</p> <p>TimeBasedHoldout - Splits the dataset chronologically into train, validation, and test sets:</p> <pre><code>import pandas as pd\nfrom omnirec.preprocess.split import TimeBasedHoldout\n\n# Split using timestamp cutoffs\ntime_split = TimeBasedHoldout(\n    validation=pd.Timestamp(year=1998, month=1, day=3),\n    test=pd.Timestamp(year=1998, month=3, day=12),\n)\ndataset = time_split.process(dataset)\n\n# Split using proportions (newest 15% for test, next 15% for validation)\ntime_split = TimeBasedHoldout(validation=0.15, test=0.15)\ndataset = time_split.process(dataset)\n\n# Split using absolute counts (newest 1000 for test, next 500 for validation)\ntime_split = TimeBasedHoldout(validation=500, test=1000)\ndataset = time_split.process(dataset)\n</code></pre> <p>The dataset is sorted by timestamp and split, with older interactions going to training and newer ones to validation/test.</p> <p>Parameters:</p> <ul> <li><code>validation</code> (float | int | pd.Timestamp): Validation set specification<ul> <li><code>float</code>: Proportion of newest interactions (0.0 to 1.0)</li> <li><code>int</code>: Absolute number of newest interactions</li> <li><code>pd.Timestamp</code>: Timestamp cutoff (interactions after this go to validation)</li> </ul> </li> <li><code>test</code> (float | int | pd.Timestamp): Test set specification (same type as validation)<ul> <li><code>float</code>: Proportion of newest interactions (0.0 to 1.0)</li> <li><code>int</code>: Absolute number of newest interactions</li> <li><code>pd.Timestamp</code>: Timestamp cutoff (interactions after this go to test)</li> </ul> </li> </ul> <p>Note: Both parameters must be of the same type. The dataset is sorted by timestamp, and interactions are split based on the specified criteria, with older interactions in training and newer ones in validation/test.</p>"},{"location":"preprocessing/#random-state","title":"Random State","text":"<p>All operations that involve randomness (sampling, splitting) use a consistent random state for reproducibility. See Getting Started &gt; Reproducibility for details how to handle random states.</p>"},{"location":"preprocessing/#data-variant-transformation","title":"Data Variant Transformation","text":"<p>The preprocessing operations transform datasets between different data variants:</p> <ul> <li><code>Subsample</code>, <code>MakeImplicit</code>, <code>CorePruning</code>, <code>TimeFilter</code>, <code>RatingFilter</code>: RawData \u2192 RawData</li> <li><code>UserHoldout</code>, <code>RandomHoldout</code>, <code>TimeBasedHoldout</code>: RawData \u2192 SplitData  </li> <li><code>UserCrossValidation</code>, <code>RandomCrossValidation</code>: RawData \u2192 FoldedData</li> </ul>"},{"location":"preprocessing/#custom-preprocessing-steps","title":"Custom Preprocessing Steps","text":"<p>You can create custom preprocessing steps by inheriting from the <code>Preprocessor</code> base class and implementing the <code>process()</code> method.</p> <p>Implementation</p> <p>Custom preprocessors must:</p> <ol> <li>Inherit from <code>Preprocessor[T, U]</code> where <code>T</code> is the input data variant and <code>U</code> is the output data variant</li> <li>Call <code>super().__init__()</code> in the constructor</li> <li>Implement the <code>process()</code> method that transforms a <code>RecSysDataSet[T]</code> to <code>RecSysDataSet[U]</code></li> </ol> <p>Available Data Variants:</p> <ul> <li><code>RawData</code>: Single DataFrame with all interactions</li> <li><code>SplitData</code>: Separate train, validation, and test DataFrames</li> <li><code>FoldedData</code>: Multiple folds for cross-validation</li> </ul> <p>Example:</p> <pre><code>from omnirec.preprocess.base import Preprocessor\nfrom omnirec.recsys_data_set import RawData, RecSysDataSet\n\nclass CustomPreprocessor(Preprocessor[RawData, RawData]):\n    def __init__(self, param1: int, param2: float) -&gt; None:\n        \"\"\"Your custom preprocessing step.\n\n        Args:\n            param1: Description of parameter 1\n            param2: Description of parameter 2\n        \"\"\"\n        super().__init__()\n        self.param1 = param1\n        self.param2 = param2\n\n    def process(self, dataset: RecSysDataSet[RawData]) -&gt; RecSysDataSet[RawData]:\n        # Log what you're doing\n        self.logger.info(f\"Applying custom preprocessing with params: {self.param1}, {self.param2}\")\n\n        # Transform the data\n        # For RawData: modify dataset._data.df directly\n        # For SplitData: access dataset._data.train, dataset._data.val, dataset._data.test\n        dataset._data.df = dataset._data.df[...]  # Your transformation logic\n\n        # Return the dataset\n        return dataset\n</code></pre> <p>Custom preprocessors can be used directly or within a <code>Pipe</code>:</p> <pre><code>from omnirec.preprocess import Pipe, CorePruning\n\npipeline = Pipe(\n    CustomPreprocessor(10, 0.5),\n    CorePruning(5)\n)\ndataset = pipeline.process(dataset)\n</code></pre>"},{"location":"preprocessing/#complete-example","title":"Complete Example","text":"<pre><code>from omnirec import RecSysDataSet\nfrom omnirec.data_loaders.datasets import DataSet\nfrom omnirec.preprocess import (\n    Pipe, Subsample, MakeImplicit, CorePruning, UserCrossValidation\n)\n\n# Load dataset\ndataset = RecSysDataSet.use_dataloader(DataSet.MovieLens100K)\n\n# Create and apply comprehensive preprocessing pipeline\npipeline = Pipe(\n    Subsample(0.1),                    # Sample 10% of interactions\n    MakeImplicit(3),                   # Convert to implicit (ratings &gt;= 3)\n    CorePruning(5),                    # Keep 5-core users and items\n    UserCrossValidation(5, 0.1)        # 5-fold CV with 10% validation\n)\n\nprocessed_dataset = pipeline.process(dataset)\n\n# Access the cross-validation folds\nfor fold_idx, split_data in processed_dataset._data.folds.items():\n    print(f\"Fold {fold_idx}:\")\n    print(f\"  Train: {len(split_data.train)} interactions\")\n    print(f\"  Validation: {len(split_data.val)} interactions\")\n    print(f\"  Test: {len(split_data.test)} interactions\")\n</code></pre>"},{"location":"slurm_integration/","title":"SLURM Integration","text":"<p>If you want to use a HPC cluster with SLURM to run your experiments, you can leverage the built-in SLURM integration of the framework. This allows you to distribute the execution of different algorithm configurations across multiple compute nodes managed by SLURM.</p>"},{"location":"slurm_integration/#enabling-slurm-integration","title":"Enabling SLURM Integration","text":"<p>To enable SLURM integration, pass the path to a SLURM script to the <code>run_omnirec()</code> function using the <code>slurm_script</code> parameter:</p> <pre><code>from omnirec.util.run import run_omnirec\n\n# Define SLURM configuration\nslurm_script = \"path/to/slurm_script.slurm\"\n\n# Run experiments with SLURM integration\nrun_omnirec(\n    datasets=dataset,\n    plan=plan,\n    evaluator=evaluator,\n    slurm_script=slurm_script\n)\n</code></pre>"},{"location":"slurm_integration/#slurm-script-template","title":"SLURM Script Template","text":"<p>The SLURM script defines the resource requirements and execution environment for each job. Here's a template:</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=1\n#SBATCH --partition=medium\n#SBATCH --output=${log_path}/%x_%j.out\n#SBATCH --error=${log_path}/%x_%j.out\n#SBATCH --job-name=${job_name}\n\n${runner}\n</code></pre>"},{"location":"slurm_integration/#template-variables","title":"Template Variables","text":"<p>The framework provides several variables that are automatically replaced when submitting jobs:</p> <p><code>${runner}</code> \u2705 Required</p> <ul> <li>The command to execute the OmniRec runner for a specific algorithm configuration</li> <li>Automatically filled by the framework</li> <li>Must be included in your script</li> </ul> <p><code>${log_path}</code> \u2b1c Optional</p> <ul> <li>Path where log files should be stored</li> <li>Useful for organizing SLURM output and error logs</li> </ul> <p><code>${job_name}</code> \u2b1c Optional</p> <ul> <li>Name of the SLURM job</li> <li>Typically reflects the algorithm and configuration being run</li> </ul> <p>Configure other SLURM directives (e.g., <code>--nodes</code>, <code>--cpus-per-task</code>, <code>--partition</code>, <code>--mem</code>) based on your cluster configuration and experiment requirements.</p>"},{"location":"slurm_integration/#how-it-works","title":"How It Works","text":"<p>When you run <code>run_omnirec()</code> with the <code>slurm_script</code> parameter, the framework will:</p> <ol> <li>Generate a separate SLURM job for each algorithm configuration in your <code>ExperimentPlan</code></li> <li>Replace template variables (<code>${runner}</code>, <code>${log_path}</code>, <code>${job_name}</code>) with appropriate values</li> <li>Submit each job to the SLURM scheduler using <code>sbatch</code></li> <li>Allow distributed execution across your HPC cluster's compute nodes</li> </ol> <p>This enables efficient parallel execution of multiple algorithm configurations, significantly reducing total experiment time on HPC clusters.</p>"}]}